{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Математика нейронных сетей*</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> <h2> !!!ВНИМАНИЕ!!! Раздел повышенной сложности </h2> </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Требования для прохождения курса (математика):\n",
    "* понятие производной и ее свойства\n",
    "* градиент\n",
    "* понятие вектора и операций над ним\n",
    "* понятие матрицы и ее свойства\n",
    "* понятие многомерной матрицы и ее свойства\n",
    "* понятие тензора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Требования для прохождения курса (программирование):\n",
    "* ООП\n",
    "* Python\n",
    "* перегрузка арифметических операций в Python\n",
    "* numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначения:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $x$ - переменная x (скаляр)\n",
    "* $\\vec x$ - вектор x\n",
    "* $X$ - матрица, многомерная матрица или тензор X (в зависимости от контекста)\n",
    "* a * b = \n",
    "* A $\\cdot$ B - поэлементное умножение\n",
    "* AB - матричное умножение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Современные фреймвоки глубокого обучения такие как PyTorch и TenzorFlow содержат в себе все необходимые иструменты для автоматического дифференцииорования. Данный раздел необходим для углубленного понимания того что происходит \"под капотом\" этих фреймворков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2> Производная одной переменной </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним таблицу основных переменных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width = 600>\n",
    "    <tr>\n",
    "        <td>Название</td>\n",
    "        <td>$f(x)$</td>\n",
    "        <td>Производная</td>\n",
    "        <td>Пример</td>\n",
    "    </tr>    \n",
    "    <tr>\n",
    "        <td>Константа</td>\n",
    "        <td>$с$</td>\n",
    "        <td>0</td>\n",
    "        <td>${\\frac{df}{dx}}10$=0</td>\n",
    "    </tr>    \n",
    "    <tr>\n",
    "        <td>Умножение на константу</td>\n",
    "        <td>$сf(x)$</td>\n",
    "        <td>$c\\frac{df}{dx}$</td>\n",
    "        <td>$\\frac{d}{dx}2x$=$2$</td>\n",
    "    </tr>    \n",
    "    <tr>\n",
    "        <td>Возведение в степень</td>\n",
    "        <td>$x^n$</td>\n",
    "        <td>$nx^{n-1}$</td>\n",
    "        <td>$\\frac{d}{dx}x^3$=$2x^2$</td>\n",
    "    </tr>    \n",
    "    <tr>\n",
    "        <td>Сумма</td>\n",
    "        <td>$f+g$</td>\n",
    "        <td>$\\frac{df}{dx} + \\frac{dg}{dx}$</td>\n",
    "        <td>$\\frac{d}{dx}(x^2+3x)$=$2x+3$</td>\n",
    "    </tr>    \n",
    "    <tr>\n",
    "        <td>Разность</td>\n",
    "        <td>$f-g$</td>\n",
    "        <td>$\\frac{df}{dx} - \\frac{dg}{dx}$</td>\n",
    "        <td>$\\frac{d}{dx}(x^2-3x)$=$2x-3$</td>\n",
    "    </tr>    \n",
    "   <tr>\n",
    "        <td>Произведение</td>\n",
    "        <td>$fg$</td>\n",
    "        <td>$f\\frac{dg}{dx} + g\\frac{df}{dx}$</td>\n",
    "        <td>$\\frac{d}{dx}(x^2*x)$=$x^2+x*2x$=$3x^2$</td>\n",
    "    </tr>    \n",
    "   <tr>\n",
    "        <td>Цепное правило</td>\n",
    "        <td>$f(g(x))$</td>\n",
    "        <td>$\\frac{df}{dg}\\frac{dg}{dx}$</td>\n",
    "        <td>$\\frac{d}{dx}ln(x^2)=\\frac{1}{x^2}2x=\\frac{2}{x}$</td>\n",
    "    </tr>    \n",
    "\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Векторы, матрицы и частные производные</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слои нейронных сетей содержат большое количество переменных для которых надо расчитывать производные. Поэтому необходимо уметь рассчитывать производные для функций многих переменных. Например чему равна производная $f(x, y) = x * y$? То есть как меняется значение функции в зависимости от изменений переменных. В первую очередь это зависит от того какую именно переменную мы изменяем? Поэтому получаем не одну производную, а две:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$вместо\\ одной\\ \\frac{df}{dx}\\ имеем\\ две\\ \\frac{\\partial f(x, y)}{\\partial x}\\ и\\ \\frac{\\partial f(x, y)}{\\partial y},\\ где\\ вместо\\ знака\\ \\frac{d}{dx}\\ используется\\ \\frac{\\partial}{\\partial x}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для функций одной переменной $\\frac{df(x)}{dx}$ и $\\frac{\\partial f(x)}{\\partial y}$ имеют одно и тоже значение, но для большей понятности лучше использовать $\\frac{df(x)}{dx}$. Для простоты для функции одной переменной $f(x)$ производную иногда записывают $f'$. Когда необходимо взять производную $\\frac{\\partial f}{\\partial x}$ все остальные переменные кроме x рассматриваются как константы. Например: $f(x, y) = 4 x^2 y^3$, необходимо вычислить $\\frac{\\partial f}{\\partial x}$ и $\\frac{\\partial f}{\\partial y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим $\\frac{\\partial f}{\\partial x}4 x^2 y^3$, т.к. расчет производной производится по x, то y-константа, а следовательно $4 y^3$ тоже константа. Тогда $\\frac{\\partial f}{\\partial x}4 x^2 y^3$ = $4 y^3\\frac{\\partial f}{\\partial x} x^2$ = $4 y^3 (2 x)$ = $8 y^3 x$. Аналогичным образом рассчитаем $\\frac{\\partial f}{\\partial y}4 x^2 y^3$ = $4 x^2\\frac{\\partial f}{\\partial y} y^3$ = $4 x^2 (3 y^2)$ = $12 x^2 y^2$. Аналогичным образом можно вычислять производные для любых других функций многих переменных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно заметить, что переменные x и y можно записать в виде вектора $\\vec{v}(x, y)$. Тогда выражение $\\frac{df(x, y)}{d \\vec{v}}$ = $(\\frac{\\partial f(x, y)}{\\partial x}, \\frac{\\partial f(x, y)}{\\partial y} )$, что на самом деле является градиентом $\\nabla f(x,y)$. Поэтому можно думать, что градиент является дифференциированием функции по вектору ее переменных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\nabla f(x,y) = \\frac{df(x, y)}{d \\vec{v}} = (\\frac{\\partial f(x, y)}{\\partial x}, \\frac{\\partial f(x, y)}{\\partial y} )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда происходит расчет производной одной переменной получается скаляр (одно число). Когда происходит расчет всех частных производных функции многих переменных получается вектор. Что получится если необходимо рассчитать сразу все частные производные нескольких функций от нескольких переменных?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, имеем $f(x, y) = 4 x^2 y^3$ и $g(x, y) = 3 x^3  + y^2$. Необходимо рассчитать все частные производные. Далее для простоты расчетов перечень параметров функции будет опускаться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial x}4 x^2 y^3$ = $4 y^3\\frac{\\partial f}{\\partial x} x^2$ = $4 y^3 (2 x)$ = $8 y^3 x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial y}4 x^2 y^3$ = $4 x^2\\frac{\\partial f}{\\partial y} y^3$ = $4 x^2 (3 y^2)$ = $12 x^2 y^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial g}{\\partial x} = \\frac{\\partial g}{\\partial x} (3x^3+y^2) = \\frac{\\partial g}{\\partial x} 3x^3 + \\frac{\\partial g}{\\partial x^2} y = 9x^2 + 0 = 9x^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial g}{\\partial y} = \\frac{\\partial g}{\\partial y} (3x^3+y^2) = \\frac{\\partial g}{\\partial y} 3x^3 + \\frac{\\partial g}{\\partial y} y^2 = 0 + 2y = 2y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если $ \\vec x = (x, y)$, то получаем:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla f = \\frac{\\partial f}{\\partial \\vec x} = (8y^3x, 12x^2y^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla g = \\frac{\\partial g}{\\partial \\vec x} = (9x^2, 2y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если $\\vec f = ( f, g)$, тогда:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d \\vec f}{d \\vec x} = \\begin{pmatrix} \\nabla f \\\\ \\nabla g \\\\ \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial f}{\\partial \\vec x} \\\\ \\frac{\\partial g}{\\partial \\vec x} \\\\ \\end{pmatrix} = \\begin{pmatrix}   \\frac{\\partial f(x,y)}{\\partial x} & \\frac{\\partial f(x,y)}{\\partial y} \\\\ \\frac{\\partial g(x,y)}{\\partial x} & \\frac{\\partial g(x,y)}{\\partial y} \\\\ \\end{pmatrix} = \\begin{pmatrix}  8𝑦^3x & 12x^2y^2 \\\\  9x^2 & 2y \\\\ \\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Как видно из приведенных расчетов $\\frac{d \\vec f}{d \\vec x}$ становится матрицей. Данная матрица называется матрицей Якоби, обозначается буквой $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J = \\begin{pmatrix} \\frac{\\partial f}{\\partial \\vec x} \\\\ \\frac{\\partial g}{\\partial \\vec x} \\\\ \\end{pmatrix} =  \\begin{pmatrix} \\nabla f \\\\ \\nabla g \\\\ \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица Якоби содержит все частные производные ф-ий $f\\ и\\ g$, от переменных $x\\ и\\ y$ в упорядоченном(матрица) виде. Обобщим построение матрицы Якоби на случай n функций и m переменных. Предположим что имеется набор функций $f_1,f_2,f_3,...f_n$ от переменных $x_1, x_2, x_3,...,x_n$, тогда матрица Якоби примет вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J \n",
    "= \n",
    "\\begin{pmatrix} \n",
    "    \\nabla f_1 \\\\ \n",
    "    \\nabla f_2 \\\\ \n",
    "    ... \\\\ \n",
    "    \\nabla f_n \\\\\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix} \n",
    "    \\frac{\\partial f_1}{\\partial \\vec x} \\\\ \n",
    "    \\frac{\\partial f_2}{\\partial \\vec x}  \\\\ \n",
    "    ... \\\\ \n",
    "    \\frac{\\partial f_n}{\\partial \\vec x}  \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} \n",
    "    \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & ... & \\frac{\\partial f_1}{\\partial x_m} \\\\ \n",
    "    \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & ... & \\frac{\\partial f_2}{\\partial x_m} \\\\ \n",
    "    ...                               & ...                               & ... & ...                               \\\\ \n",
    "    \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & ... & \\frac{\\partial f_n}{\\partial x_m} \\\\\n",
    "\\end{pmatrix}\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из приведенной формулы матрица Якоби имеет размер n * m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обобщая все вышесказанное можно сделать вывод, что операцию дифференциирования является всего лишь оператором, результаты работы которого приведены в таблице ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=600>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Скаляр (x)</td>\n",
    "        <td>Вектор ($\\vec x$)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Скаляр ($f$)</td>\n",
    "        <td>$ \\frac{df}{dx} $</td>\n",
    "        <td>$ \\frac{\\partial f}{\\partial \\vec x} = (\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, ... , \\frac{\\partial f}{\\partial x_m})$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Вектор ($\\vec f$)</td>\n",
    "        <td>$\n",
    "            \\frac{\\partial \\vec f}{\\partial x} = \n",
    "            \\begin{pmatrix} \n",
    "                \\frac{\\partial f_1}{\\partial x} \\\\ \n",
    "                \\frac{\\partial f_2}{\\partial x}  \\\\ \n",
    "                ... \\\\ \n",
    "                \\frac{\\partial f_n}{\\partial x}  \\\\\n",
    "            \\end{pmatrix}$</td>\n",
    "        <td>$\n",
    "            \\frac{\\partial \\vec f}{\\partial \\vec x} = \n",
    "            \\begin{pmatrix} \n",
    "                \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & ... & \\frac{\\partial f_1}{\\partial x_m} \\\\ \n",
    "                \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & ... & \\frac{\\partial f_2}{\\partial x_m} \\\\ \n",
    "                ...                               & ...                               & ... & ...                               \\\\ \n",
    "                \\frac{\\partial f_n}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & ... & \\frac{\\partial f_n}{\\partial x_m} \\\\\n",
    "            \\end{pmatrix}$</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Вычисление частных производных в векторно-матричной форме </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим имеется множество (вектор) $\\vec f = (f_1, f_2, ... f_n)$ и множество (вектор) переменных $\\vec x = (x_1, x_2, ... x_m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Вычисление частных производных для констант в векторно-матричной форме </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Если $f_1=c_1,\\ f_2=с_2,\\ ...,\\ f_n=c_n$, тогда:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=600>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Скаляр (x)</td>\n",
    "        <td>Вектор ($\\vec x$)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Скаляр ($f$)</td>\n",
    "        <td>$ \\frac{df}{dx} $</td>\n",
    "        <td>$ \\frac{\\partial f}{\\partial \\vec x} = (0, 0, ... , 0)$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Вектор ($\\vec f$)</td>\n",
    "        <td>$\n",
    "            \\frac{\\partial \\vec f}{\\partial x} = \n",
    "            \\begin{pmatrix} \n",
    "                0 \\\\ \n",
    "                0 \\\\ \n",
    "                ... \\\\ \n",
    "                0 \\\\\n",
    "            \\end{pmatrix}$</td>\n",
    "        <td>$\n",
    "            \\frac{\\partial \\vec f}{\\partial \\vec x} = \n",
    "            \\begin{pmatrix} \n",
    "                0 & 0 & ... & 0 \\\\ \n",
    "                0 & 0 & ... & 0 \\\\ \n",
    "                ... & ... & ... & ...\\\\ \n",
    "                0 & 0 & ... & 0 \\\\\n",
    "            \\end{pmatrix}$</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Вычисление частных производных для скаляного умножения в векторно-матричной форме</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если $f_1=k_1g_1(\\vec x),\\ f_2=k_2g_2(\\vec x),\\ ...,\\ f_n=k_ng_n(\\vec x)$, тогда:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=800>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Скаляр (x)</td>\n",
    "        <td>Вектор ($\\vec x$)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Скаляр ($f$)</td>\n",
    "        <td>$ k\\frac{df}{dx} $</td>\n",
    "        <td>$ \\frac{\\partial f}{\\partial \\vec x} = \n",
    "            (k\\frac{\\partial g}{\\partial x_1}, \n",
    "             k\\frac{\\partial g}{\\partial x_2}, \n",
    "             ... , \n",
    "             k\\frac{\\partial g}{\\partial x_m})$ \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Вектор ($\\vec f$)</td>\n",
    "        <td>$\n",
    "            \\frac{\\partial \\vec f}{\\partial x} = \n",
    "            \\begin{pmatrix} \n",
    "                 k_1\\frac{\\partial g_1}{\\partial x} \\\\\n",
    "                 k_2\\frac{\\partial g_2}{\\partial x} \\\\\n",
    "                 ... \\\\\n",
    "                 k_n\\frac{\\partial g_n}{\\partial x} \\\\\n",
    "            \\end{pmatrix}$\n",
    "        </td>\n",
    "        <td>$\n",
    "            \\frac{\\partial \\vec f}{\\partial \\vec x} = \n",
    "            \\begin{pmatrix} \n",
    "                k_1\\frac{\\partial f_1}{\\partial x_1} & k_1\\frac{\\partial f_1}{\\partial x_2} & ... & k_1\\frac{\\partial f_1}{\\partial x_m} \\\\ \n",
    "                k_2\\frac{\\partial f_2}{\\partial x_1} & k_2\\frac{\\partial f_1}{\\partial x_2} & ... & k_2\\frac{\\partial f_2}{\\partial x_m} \\\\ \n",
    "                ...                               & ...                               & ... & ...                               \\\\ \n",
    "                k_n\\frac{\\partial f_n}{\\partial x_1} & k_n\\frac{\\partial f_1}{\\partial x_2} & ... & k_n\\frac{\\partial f_n}{\\partial x_m} \\\\\n",
    "            \\end{pmatrix}$</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Вычисление частных производных функции одной переменной</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если $f_1=g(x_1),\\ f_2=g(x_2),\\ ...,\\ f_n=g(x_n)$, то есть $\\vec f = g(\\vec x)$, например $\\vec f = sin(\\vec x)$ или $\\vec f = e^{\\vec x}$, тогда:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=800>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Скаляр (x)</td>\n",
    "        <td>Вектор ($\\vec x$)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Скаляр ($f$)</td>\n",
    "        <td>$ \\frac{df}{dx} $</td>\n",
    "        <td>$ \\frac{\\partial f}{\\partial \\vec x} = \n",
    "            (\\frac{\\partial f}{\\partial g}\\frac{\\partial g}{\\partial x_1}, \n",
    "             \\frac{\\partial f}{\\partial g}\\frac{\\partial g}{\\partial x_2}, \n",
    "             ... , \n",
    "             \\frac{\\partial f}{\\partial g}\\frac{\\partial g}{\\partial x_m})$ \n",
    "        </td>\n",
    "    </tr>    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Вычисление частных производных для бинарных операций в векторно-матричной форме</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бинарной $\\circ$ считается такая операция, что если существуют функции $f(g, p)$ (например $f(x)=g(x)+p(x)=sin(x)+cos(x)$) то:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial g}{\\partial x} \\circ \\frac{\\partial p}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Многие операции, такие как сложение, вычитание, умножение на скаляр являются бинарными. Бинарные операции имеют ондинаковый алгорит дифференциирования в матрично-векторной форме. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если $f_1=g_1(\\vec x) \\circ p_1(\\vec x),\\ f_2=g_2(\\vec x) \\circ p_2(\\vec x),\\ ...,\\ f_n=g_n(\\vec x) \\circ p_n(\\vec x)$, где $\\circ$ - бинарная операция (+, - и т.д.), тогда:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=1000>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Скаляр (x)</td>\n",
    "        <td>Вектор ($\\vec x$)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Скаляр ($f$)</td>\n",
    "        <td>$ \\frac{\\partial g}{\\partial x} \\circ \\frac{\\partial p}{\\partial x} $</td>\n",
    "        <td>$ \\frac{\\partial f}{\\partial \\vec x} = \n",
    "            (\\frac{\\partial g}{\\partial x_1} \\circ \\frac{\\partial p}{\\partial x_1}, \n",
    "             \\frac{\\partial g}{\\partial x_2} \\circ \\frac{\\partial p}{\\partial x_2}, \n",
    "             ... , \n",
    "             \\frac{\\partial g}{\\partial x_m} \\circ \\frac{\\partial p}{\\partial x_m})$ \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Вектор ($\\vec f$)</td>\n",
    "        <td>$\n",
    "            \\frac{\\partial \\vec f}{\\partial x} = \n",
    "            \\begin{pmatrix} \n",
    "                 \\frac{\\partial g_1}{\\partial x} \\circ \\frac{\\partial p_1}{\\partial x} \\\\\n",
    "                 \\frac{\\partial g_2}{\\partial x} \\circ \\frac{\\partial p_2}{\\partial x} \\\\\n",
    "                 ... \\\\\n",
    "                 \\frac{\\partial g_n}{\\partial x} \\circ \\frac{\\partial p_n}{\\partial x} \\\\\n",
    "            \\end{pmatrix}$\n",
    "        </td>\n",
    "        <td>$\n",
    "            \\frac{\\partial \\vec f}{\\partial \\vec x} = \n",
    "            \\begin{pmatrix} \n",
    "                \\frac{\\partial g_1}{\\partial x_1} \\circ \\frac{\\partial p_1}{\\partial x_1} \n",
    "                    & \\frac{\\partial g_1}{\\partial x_2} \\circ \\frac{\\partial p_1}{\\partial x_2} \n",
    "                    & ... \n",
    "                    & \\frac{\\partial g_1}{\\partial x_m} \\circ \\frac{\\partial p_1}{\\partial x_m} \\\\ \n",
    "                \\frac{\\partial g_2}{\\partial x_1} \\circ \\frac{\\partial p_2}{\\partial x_1} \n",
    "                    & \\frac{\\partial g_2}{\\partial x_2} \\circ \\frac{\\partial p_2}{\\partial x_2} \n",
    "                    & ... \n",
    "                    & \\frac{\\partial g_2}{\\partial x_m} \\circ \\frac{\\partial p_2}{\\partial x_m} \\\\ \n",
    "                ...                               \n",
    "                    &...\n",
    "                    &...\n",
    "                    &...                                    \\\\ \n",
    "                \\frac{\\partial g_n}{\\partial x_1} \\circ \\frac{\\partial p_n}{\\partial x_1} \n",
    "                    & \\frac{\\partial g_n}{\\partial x_2} \\circ \\frac{\\partial p_n}{\\partial x_2} \n",
    "                    & ... \n",
    "                    & \\frac{\\partial g_n}{\\partial x_m} \\circ \\frac{\\partial p_n}{\\partial x_m} \\\\ \n",
    "            \\end{pmatrix}$</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачастую фунции $g_i = g_i(\\vec x) = g_i(x_1, x_2, ..., x_n)$ и $p_i = p_i(\\vec x) = p_i(x_1, x_2, ..., x_n)$ имеют не весь набор параметров $\\vec x$, а только $x_i$.  Тогда $g_i = g_i(x_i)$ и $p_i = p_i(x_i)$. При дифференциировании $f_i$ по $x_k$ получим $\\frac{\\partial}{\\partial x_k}f_i(x_i)$ = $\\frac{\\partial}{\\partial x_k}g_i(x_i) = 0$, потому что дифференциирование функции по любой переменной не содержащейся в этой функции обратит ее производную в 0. Тогда матрица примет вид "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial \\vec f}{\\partial \\vec x} = \n",
    "            \\begin{pmatrix} \n",
    "                \\frac{\\partial g_1}{\\partial x_1} \\circ \\frac{\\partial p_1}{\\partial x_1} \n",
    "                    & 0 \n",
    "                    & ... \n",
    "                    & 0 \\\\ \n",
    "                0\n",
    "                    & \\frac{\\partial g_2}{\\partial x_2} \\circ \\frac{\\partial p_2}{\\partial x_2} \n",
    "                    & ... \n",
    "                    & 0 \\\\ \n",
    "                ...                               \n",
    "                    &...\n",
    "                    &...\n",
    "                    &...                                    \\\\ \n",
    "                0 \n",
    "                    &0\n",
    "                    & ... \n",
    "                    & \\frac{\\partial g_n}{\\partial x_m} \\circ \\frac{\\partial p_n}{\\partial x_m} \\\\ \n",
    "            \\end{pmatrix}  =   diag(\\frac{\\partial g_1}{\\partial x_1} \\circ \\frac{\\partial p_1}{\\partial x_1} ,\\frac{\\partial g_2}{\\partial x_2} \\circ \\frac{\\partial p_2}{\\partial x_2},...,\\frac{\\partial g_n}{\\partial x_m} \\circ \\frac{\\partial p_n}{\\partial x_m})\n",
    "          $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например имеем функцию $\\vec f_i(x, w) = g_i(x_i) + p_i(w_i) = x_i + w_i$, тогда "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial}{\\partial w_i}(g_i(x_i) + p_i(w_i)) = \\frac{\\partial}{\\partial w_i}w_i+\\frac{\\partial}{\\partial w_i}x_i=1 + 0 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial}{\\partial x_i}(g_i(x_i) + p_i(w_i)) = \\frac{\\partial}{\\partial x_i}w_i+\\frac{\\partial}{\\partial x_i}x_i=0 + 1 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И получается, что $\\frac{\\partial \\vec f}{\\partial (\\vec x = (x, w))} = \n",
    "\\begin{pmatrix} \n",
    "                1 & 0 \\\\\n",
    "                0 & 1 \\\\\n",
    "\\end{pmatrix} = diag(1, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсюда можно рассчитать несколько частных случаев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $f = w + x$, $\\frac{\\partial (w+x)}{\\partial w} = diag(...\\frac{\\partial (w_i+x_i)}{\\partial w_i}...) = diag(\\vec 1) = I, \\frac{\\partial (w+x)}{\\partial x} = I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $f = w - x$, $\\frac{\\partial (w-x)}{\\partial w} = diag(...\\frac{\\partial (w_i-x_i)}{\\partial w_i}...) = diag(\\vec 1) = I, \\frac{\\partial (w-x)}{\\partial x} = -I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $f = w * x$, $\\frac{\\partial (w*x)}{\\partial w} = diag(...\\frac{\\partial (w_i*x_i)}{\\partial w_i}...) = diag(\\vec x), \\frac{\\partial (w*x)}{\\partial x} = w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $f = w / x$, $\\frac{\\partial ( w/x)}{\\partial w} = diag(...\\frac{\\partial (w_i/x_i)}{\\partial w_i}...) = diag(... \\frac{1}{x_i} ...), \\frac{\\partial (w/x)}{\\partial x} = diag(... \\frac {-w_i}{x_i^2} ...)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда необходимо поэлементово умножить вектор на число или прибавить к каждому элементу вектора число, то данное действие можно представить с помощью увеличения размерности. \n",
    "Для этого данное число расширяется до размеров заданного вектора, потом происходит необходимая операция. Пример $\\vec x=(1,2,3)$, необходимо умножить на $k=2$. Тогда необходимо расширить k=2 до вектора $\\vec k=(2, 2, 2)$ и произвести умножение $\\vec k \\vec x = (1 * 2, 2 * 2, 2 * 3) = (2, 4, 6)$. Данную операцию можно записать в виде $y = x * (I * k)$.\n",
    "Аналогичные действия можно сделать и для поэлементного сложения и подобных операций.  Данная операция подчиняется правилу $\\frac{\\partial y}{\\partial x} = diag(... \\frac{\\partial}{\\partial x_i}(f_i(x_i) \\circ g_i(z)) ...)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Вычисление частных производных суммы </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суммирование элементов часто встречающаяся операция в глубоком обучении. Предположим имеем $f(\\vec x) = \\sum_{i=1}^n g_i(\\vec x)$. Необходимо посчитать, чему будет равен $\\frac{\\partial f}{\\partial \\vec x}$. Как описывалось выше \n",
    "$$\\frac{\\partial f}{\\partial \\vec x} = \\big(\\frac{\\partial g}{\\partial x_1}, \\frac{\\partial g}{\\partial x_2}, ... , \\frac{\\partial g}{\\partial x_m} \\big)\n",
    "=\\big(\\frac{\\partial}{\\partial x_1} \\sum\\limits_{i=1}^ng_i(\\vec x), \\frac{\\partial}{\\partial x_2} \\sum\\limits_{i=1}^ng_i(\\vec x), ..., \\frac{\\partial}{\\partial x_n} \\sum\\limits_{i=1}^n g_i(\\vec x)\\big) = \n",
    "\\big(\\sum\\limits_{i=1}^n \\frac{\\partial g_i(\\vec x)}{\\partial x_1}, \\sum\\limits_{i=1}^n \\frac{\\partial g_i(\\vec x)}{\\partial x_2}, ..., \\sum\\limits_{i=1}^n \\frac{\\partial g_i(\\vec x)}{\\partial x_n}\\big)\n",
    "$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим случай, когда $f = \\sum x$. Тогда $g_i(\\vec x) = x_i$. Градиент примет вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial f}{\\partial \\vec x} = \n",
    "\\big(\\sum\\limits_{i=1}^n \\frac{\\partial g_i(\\vec x)}{\\partial x_1}, \\sum\\limits_{i=1}^n \\frac{\\partial g_i(\\vec x)}{\\partial x_2}, ..., \\sum\\limits_{i=1}^n \\frac{\\partial g_i(\\vec x)}{\\partial x_n}\\big)= \n",
    "\\big(\\sum\\limits_{i=1}^n \\frac{\\partial x_i}{\\partial x_1}, \\sum\\limits_{i=1}^n \\frac{\\partial x_i}{\\partial x_2}, ..., \\sum\\limits_{i=1}^n \\frac{\\partial x_i}{\\partial x_n}  \\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "т.к. $\\frac{\\partial}{\\partial x_j}x_i = 0$ при $i \\neq j$, то выражение упрощается до"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial f}{\\partial \\vec x} = \n",
    "\\big(\\sum\\limits_{i=1}^n \\frac{\\partial x_i}{\\partial x_1}, \\sum\\limits_{i=1}^n \\frac{\\partial x_i}{\\partial x_2}, ..., \\sum\\limits_{i=1}^n \\frac{\\partial x_i}{\\partial x_n}  \\big) = \n",
    "(1, 1, ... 1) = \\vec 1 ^ T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо помнить, что при матричном дифференциировании сложной функции важны верные размеры вектора, поэтому вектор $(1, 1, ... 1)$ обязательно должен быть транспонированным $\\vec 1 ^ T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим другой пример $f(x, k) = \\sum(k \\vec x)$, где k - является скаляром"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial f}{\\partial \\vec x} = \n",
    "\\big(\\sum\\limits_{i=1}^n \\frac{\\partial}{\\partial x_1}kx_i, \\sum\\limits_{i=1}^n \\frac{\\partial}{\\partial x_2}kx_i, ..., \\sum\\limits_{i=1}^n \\frac{\\partial}{\\partial x_n}kx_i \\big) =\n",
    "\\big( \\frac{\\partial}{\\partial x_1}kx_1, \\frac{\\partial}{\\partial x_2}kx_2, ..., \\frac{\\partial}{\\partial x_n}kx_n \\big) = \n",
    "(k, k, ..., k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дифференциирование относительно скаляра k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial f}{\\partial k} = \n",
    "\\frac{\\partial}{\\partial k} \\sum\\limits_{i=1}^n  kx_i = \n",
    "\\sum\\limits_{i=1}^n \\frac{\\partial}{\\partial k}  kx_i = \n",
    "\\sum\\limits_{i=1}^n x_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Вычисление частной производной функций max и min</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция min имеет вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "min(f, g)=\n",
    "\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   f \\ ,если\\ f < g\\\\\n",
    "   g \\ ,если\\ f \\geq g\\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция max имеет вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "max(f, g)=\n",
    "\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   f \\ ,если\\ f > g\\\\\n",
    "   g \\ ,если\\ f \\leq g\\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Где $f, g$ - некоторые функции. Тогда производные данных функций примут вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\frac{\\partial}{\\partial x}min(f, g)=\n",
    "\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   \\frac{\\partial f}{\\partial x} \\ ,если\\ f < g\\\\\n",
    "   \\frac{\\partial g}{\\partial x} \\ ,если\\ f \\geq g\\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\frac{\\partial}{\\partial x}max(f, g)=\n",
    "\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   \\frac{\\partial f}{\\partial x} \\ ,если\\ f > g\\\\\n",
    "   \\frac{\\partial g}{\\partial x} \\ ,если\\ f \\leq g\\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Цепное правило. Дифференциирование сложных функций.</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для дифференциирования сложных функций в матрично-векторной форме необходимо обобщить правила дифференциирования для одной и многих переменных. В литературе по высшей математике существуют три операции, которые называются \"цепное правило\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* дифференциирование сложной функции одной переменной\n",
    "* полное дифференциирование сложной функции одной переменной\n",
    "* дифференциирование сложной функции многих переменных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной задачи, которые поможет решить применение цепного правила для дифференциирования сложных функций многих переменных для нейронных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Унификация всех операций. Все операции будут представлены однотипными кострукциями, которые легко запрограммировать.\n",
    "* Ускорение расчетов. Цепное правило можно запрограммировать с помощью метода \"разделяй и властвуй\" (динамического программирования), что значительно экономит вычислительные ресурсы.\n",
    "* Простота. С программной точки зрения системы состоящие их единообразных элементов проще поддерживать, расширять и понимать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Дифференциирование сложной функции одной переменной</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сложная функция – функция от функции. Если f – функция от g, т.е. f(g), а g, в свою очередь, – функция от х, т.е. g(х), то функция f(x) = f(g(x)) называется сложной функцией (или композицией, или суперпозицией функций) от х."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим операцию, которую называют \"цепное правило\" или дифференциирование сложной функции одной переменной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{df}{dx} = \\frac{df}{dg} \\frac{dg}{dx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например имеем $f(x) = cos(x^3)$ необходимо вычислить $\\frac{df}{dx}$. Для этого необходимо сделать следующие действия:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Определим порядок действий $f(x)=cos(x^3)$: $(x^3) \\rightarrow cos(x^3)$\n",
    "2. Обозначим $g=g(x)=x^3$. Тогда f(x) = cos(g(x)), $ \\frac{df}{dx} = \\frac{d}{dg}cos(g) \\frac{d}{dx}x^3$\n",
    "3. $\\frac{d}{dg}cos(g) = -cos(g)$\n",
    "   $\\frac{d}{dx}x^3 = 3x^2$\n",
    "4. если $\\frac{d}{dg}cos(g) = -cos(g)$, а $g=g(x)=x^3$, то $\\frac{d}{dg}cos(g) = -cos(g) = -cos(x^3)$\n",
    "5. $ \\frac{df}{dx} = \\frac{d}{dg}cos(g) \\frac{d}{dx}x^3 = -cos(g) * (3x^2) = -cos(x^3) * (3x^2) = -3x^2cos(x^3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформулируем общий алгоритм применения цепного правила при дифференциировании функции одной переменной:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Определить порядок операций.\n",
    "2. Сделать замены переменных.\n",
    "3. Вычислить все частные производные.\n",
    "4. Сделать все обратные подстановки. \n",
    "5. Получить результат, перемножив все производные, полученные на шаге 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим необходимо рассчитать более сложную функцию вида $f=f(x)=ln(sin(x^3)^2)$. В таком случае необхожимо выявить порядок вызова функций и многократно применить цепное правило. Для приведенного примера будем иметь сдедующий порядок функций: логарифм -> возведение в квадрат-> синус -> возведение в куб. Тогда данную функцию можно переписать в следующий вид: $f=f(x)=ln(sqr(sin(cube(x))))$. Вычислим $\\frac{df}{dx}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Определим порядок операций: $cube(x) \\rightarrow sin(cube) \\rightarrow sqr(sin) \\rightarrow ln(sqr)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Произведем замену переменных\n",
    "- $g_1 = cube(x) = x^3$ \n",
    "- $g_2 = sin(g_1)$\n",
    "- $g_3 = sqr(g_2) = g_2^2$\n",
    "- $g_4 = ln(g3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда $\\frac{df}{dx}$ примет вид $\\frac{df}{dx}=\\frac{dg_4}{dg_3}\\frac{dg_3}{dg_2}\\frac{dg_2}{dg_1}\\frac{dg_1}{dx}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Вычислим частные призводные\n",
    "- $\\frac{dg_1}{dx} = \\frac{dx^3}{dx} = 3x^2$\n",
    "- $\\frac{dg_2}{dg_1} = \\frac{d}{dg_1}sin(g_1) = cos(g_1)$\n",
    "- $\\frac{dg_3}{dg_2} = \\frac{d}{dg_2}g_2^2 = 2g_2$\n",
    "- $\\frac{dg_4}{dg_3} = \\frac{d}{dg_3}ln(g_3) = \\frac{1}{g_3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Сделаем обратную подстановку\n",
    "- $\\frac{dg_1}{dx} = 3x^2$\n",
    "- $\\frac{dg_2}{dg_1} = cos(g_1) = cos(x^3)$\n",
    "- $\\frac{dg_3}{dg_2} = 2g_2 = 2sin(g_1) = 2sin(x^3)$\n",
    "- $\\frac{dg_4}{dg_3} = \\frac{1}{g_3} = \\frac{1}{g_2^2} = \\frac{1}{sin(g_1)^2}=\\frac{1}{sin(x_3)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Собираем результат\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{df}{dx}=\\frac{dg_4}{dg_3}\\frac{dg_3}{dg_2}\\frac{dg_2}{dg_1}\\frac{dg_1}{dx}=3x^2*cos(x^3)*2sin(x^3)*\\frac{1}{sin(x_3)^2}=\\frac{6x^2cos(x^3)}{sin(x^3)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрие еще один пример. Предположим имеем $f=f(x)=x+x^2$. Легко понять что производная данной функции $\\frac{df}{dx} = 2x + 1$. Но попробуем расчитать эту производную с помощью цепного правила. $f=f(x)=x+x^2=sum(x, sqr(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Определим порядок операций: $sqr(x) \\rightarrow sum(x, sqr(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Произведем замену переменных\n",
    "- $g_1 = g_1(x) = x^2$\n",
    "- $g_2 = g_2(x, g_1) = x + g_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Вычислим частные производные\n",
    "- $\\frac{dg_1}{dx} = 2x$\n",
    "- $\\frac{dg_2}{dg_1} = \\frac{dx}{dg_1} + \\frac{dg_1}{dg_1} = 0 + 1 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Обратная подстановка не требуется"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Получим результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{df}{dx}=\\frac{dg_2}{dg_1}\\frac{dg_1}{dx}=2x * 1 = 2x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе вычисления цепного правила получен ответ 2x, хотя правильный ответ 2x+1. Причина данной ошибки заключается в том, что при дифференциировании $\\frac{dg_2}{dg_1}$ $\\frac{dx}{dg_1} = 0$. Но подобное вычисление не возможно провести, т.к. $dg_1=x^2$ и зависит от x. Поэтому дифференцииорование сложной функции одной переменной применимо только в тех случаях, когда при замене переменных не возникает скрытых зависимостей дифференциирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Полное дифференциирование сложной функции одной переменной</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения описанной выше проблемы существует правило полного дифференциирования сложной функции одной переменной. Данное правило утверждает, что для вычисления $\\frac{d}{dx}f(x, g_1(x), g_2(x), ..., g_n(x))$ необходимо просуммировать все возможные производные $\\frac{df}{dx},\\ \\frac{dg_1}{dx},\\ \\frac{dg_2}{dx},\\ ,...,\\ \\frac{dg_n}{dx}$ зависящие от x. Тогда формула правила полного дифференциирования функции одной переменной примет вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial f(x, g_1, g_2, ..., g_n)}{\\partial x}=\\frac{\\partial f}{\\partial x} + \\frac{\\partial f}{\\partial g_1}\\frac{\\partial g_1}{\\partial x} + \\frac{\\partial f}{\\partial g_2}\\frac{\\partial g_2}{\\partial x} + ... + \\frac{\\partial f}{\\partial g_n}\\frac{\\partial g_n}{\\partial x} =\\frac{\\partial f}{\\partial x} + \\sum\\limits_{i=1}^n \\frac{\\partial f}{\\partial g_i}\\frac{\\partial g_i}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты записи предположим $x=g_1(x)$, тогда правило упростится до:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial f(g_1, g_2, ..., g_{n+1})}{\\partial x}= \\sum\\limits_{i=1}^{n+1} \\frac{\\partial f}{\\partial g_i}\\frac{\\partial g_i}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следует отметить, что ф-ии $g_i$ могут иметь вид $g_i(x, y, z, ...)$, но в данном случае имеет только значение, что они зависят от $x$. Как видно из приведенных правил полное дифференциирование сводится к суммированию цепных правил отдельных функций зависящих от $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продифференциируем $f(x) = x + x^2$, приведенную в предыдущей главе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Определим порядок операций $x^2 \\rightarrow x + x^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Сделаем замены переменных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $g_1(x) = x^2$\n",
    "- $g_2(x, g_1) = x + g_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Вычислить все частные производные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\frac{\\partial g_1}{\\partial x} = 2x$\n",
    "- $\\frac{\\partial g_2}{\\partial g_1} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Делать обратные подстановки не нужно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Дополнительные вычисление не нужны"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Получим результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial f}{\\partial x} = \\frac{\\partial g_2}{\\partial g_1}\\frac{\\partial g_1}{\\partial x} + \\frac{\\partial g_1}{\\partial x}=2x+1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получен верный ответ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим ещё один пример: необходимо вычислить производную $f(x) = cos (x + x^3)$.Необходимо вычислить производную $f(x) = cos (x + x^3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Определим порядок операций. $x^3 \\rightarrow sum(x, x^3) \\rightarrow cos(sum(x, x^3))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Произведем замену переменных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $g_1(x) = x^3$ \n",
    "- $g_2(x, g_1) = x+g_1$\n",
    "- $g_3(g_2) = cos(g_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Вычислим частные производные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\frac{\\partial g_1}{\\partial x} = 3x^2$\n",
    "- $\\frac{\\partial g_2}{\\partial g_1} = 0 + 1$\n",
    "- $\\frac{\\partial g_3}{\\partial g2} = -sin(g_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Сделаем обратную подстановку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial g_3}{\\partial g2} = -sin(g_2) = -sin(x+x^3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Произведем промежуточные вычисления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\frac{\\partial g_1}{\\partial x} = 3x^2$\n",
    "- $\\frac{\\partial g_2}{\\partial x} = \\frac{\\partial x}{\\partial x} + \\frac{\\partial g_2}{\\partial g_1}\\frac{\\partial g_1}{\\partial x} = 1 + 3x^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Получим результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial f}{\\partial x} = \\frac{\\partial g_3}{\\partial x} + \\frac{\\partial g_3}{\\partial g_2}\\frac{\\partial g_2}{\\partial x} = 0 - sin(g_2)\\frac{\\partial g_2}{\\partial x} = -sin(x+x^3)(1+3x^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из приведенного примера для расчета некоторых производных используются производные рассчитанные на предъидущих шагах (динамическое программирование). Это даёт выигрыш в скорости расчетов на вычислительной технике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформулируем общий алгоритм применения цепного правила для полного дифференциировании функции одной переменной:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Определить порядок операций.\n",
    "2. Сделать замены переменных.\n",
    "3. Вычислить все частные производные.\n",
    "4. Сделать все обратные подстановки. \n",
    "5. Произведем промежуточные вычисления, перемножив соответствующие производные, полученные на шаге 4.\n",
    "6. Сложить все результаты, полученные на шаге 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем более сложный пример $f(x)=x^x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Определим порядок операций: $x \\rightarrow x^x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Сделаем замену переменных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $g_1(x) = x$\n",
    "- $g_2(x) = x$\n",
    "- $g_3(g_1, g_2) = g_1^{g_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Вычислим частные производные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\frac{\\partial g_1}{\\partial x} = 1$\n",
    "- $\\frac{\\partial g_2}{\\partial x} = 1$\n",
    "- $\\frac{\\partial g_2}{\\partial g_1} = g_1^{g_2}ln(g_1)$\n",
    "- $\\frac{\\partial g_1}{\\partial g_2} = g_2g_1^{g_2-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Производные $\\frac{\\partial g_1}{\\partial g_2}$ и $\\frac{\\partial g_2}{\\partial g_1}$ берутся, т.к. каждая из функций $g_1$, $g_2$ зависит от x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. сделаем обратные подстановки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\frac{\\partial g_2}{\\partial g_1} = g_1^{g_2}ln(g_1) = x^{x}ln(x)$\n",
    "- $\\frac{\\partial g_1}{\\partial g_2} = g_2g_1^{g_2-1} = xx^{x-1} = x^x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Произведем промежуточные вычисления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\frac{\\partial g1}{\\partial x} = \\frac{\\partial g1}{\\partial g2}\\frac{\\partial g2}{\\partial x} = x^{x}ln(x) * 1 = x^{x}ln(x)$\n",
    "- $\\frac{\\partial g2}{\\partial x} = \\frac{\\partial g2}{\\partial g1}\\frac{\\partial g1}{\\partial x} = x^{x} * 1 = x^{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Получим результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial f}{\\partial x} = \\frac{\\partial g_2}{\\partial x} + \\frac{\\partial g_1}{\\partial x} = x^{x}+x^{x}ln(x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что формулу $\\frac{\\partial f(g_1, g_2, ..., g_{n+1})}{\\partial x}= \\sum\\limits_{i=1}^{n+1} \\frac{\\partial f}{\\partial g_i}\\frac{\\partial g_i}{\\partial x}$ можно записать в более лаконичной векторной форме:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial f(g_1, g_2, ..., g_{n+1})}{\\partial x}= \\frac{\\partial f}{\\partial \\vec g}\\frac{\\partial \\vec g}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Дифференциирование сложных функций многих переменных в векторно-матричной форме</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выше были рассмотрены правила дифференциирования сложной функции одной переменной в векторно-матричной форме. Для получения общего правила дифференциирования сложных функций многих переменных в векторно-матричной форме необходимо рассмотреть пример. Предположим $\\vec f = \\vec f (x)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{pmatrix} \n",
    "    f_1(x) \\\\\n",
    "    f_2(x) \\\\\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix} \n",
    "    cos(x^3)\\\\\n",
    "    ln(2x)  \\\\\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Произведём замену переменных:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{pmatrix} \n",
    "    g_1(x) \\\\\n",
    "    g_2(x) \\\\\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix} \n",
    "    x^3\\\\\n",
    "    2x  \\\\\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{pmatrix} \n",
    "    f_1(x) \\\\\n",
    "    f_2(x) \\\\\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix} \n",
    "    cos(g_1)\\\\\n",
    "    ln(g_2)  \\\\\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда цепное правило примет вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial \\vec f}{\\partial x} = \n",
    "\\begin{pmatrix} \n",
    "    \\frac{\\partial f_1(g)}{\\partial x} \\\\\n",
    "    \\frac{\\partial f_2(g)}{\\partial x} \\\\\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix} \n",
    "    \\frac{\\partial f_1}{\\partial g_1}\\frac{\\partial g_1}{\\partial x} + \\frac{\\partial f_1}{\\partial g_2}\\frac{\\partial f_2}{\\partial x} \\\\\n",
    "    \\frac{\\partial f_2}{\\partial g_1}\\frac{\\partial g_1}{\\partial x} + \\frac{\\partial f_2}{\\partial g_2}\\frac{\\partial f_2}{\\partial x} \\\\\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix} \n",
    "    -sin(g1)*3x^2 + 0 \\\\\n",
    "    0 + \\frac{1}{g_2}*2 \\\\\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix} \n",
    "    -3x^2sin(x^3) \\\\\n",
    "    \\frac{1}{x} \\\\\n",
    "\\end{pmatrix}\n",
    "$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы получить цепное правило в векторно-матричеой форме необходимо понять как представить \n",
    "$\\begin{pmatrix} \n",
    "    \\frac{\\partial f_1}{\\partial g_1}\\frac{\\partial g_1}{\\partial x} + \\frac{\\partial f_1}{\\partial g_2}\\frac{\\partial f_2}{\\partial x} \\\\\n",
    "    \\frac{\\partial f_2}{\\partial g_1}\\frac{\\partial g_1}{\\partial x} + \\frac{\\partial f_2}{\\partial g_2}\\frac{\\partial f_2}{\\partial x} \\\\\n",
    "\\end{pmatrix}$\n",
    "в виде вектрно-матричных операций.\n",
    "$\\begin{pmatrix} \n",
    "    \\frac{\\partial f_1}{\\partial g_1}\\frac{\\partial g_1}{\\partial x} + \\frac{\\partial f_1}{\\partial g_2}\\frac{\\partial f_2}{\\partial x} \\\\\n",
    "    \\frac{\\partial f_2}{\\partial g_1}\\frac{\\partial g_1}{\\partial x} + \\frac{\\partial f_2}{\\partial g_2}\\frac{\\partial f_2}{\\partial x} \\\\\n",
    "\\end{pmatrix}$\n",
    "можно представить в виде произведения двух матриц\n",
    "$\\begin{pmatrix} \n",
    "    \\frac{\\partial f_1}{\\partial g_1} & \\frac{\\partial f_1}{\\partial g_2} \\\\\n",
    "    \\frac{\\partial f_2}{\\partial g_1} & \\frac{\\partial f_2}{\\partial g_2} \\\\\n",
    "\\end{pmatrix}$\n",
    "$\\begin{pmatrix} \n",
    "    \\frac{\\partial g_1}{\\partial x} \\\\\n",
    "    \\frac{\\partial g_2}{\\partial x} \\\\\n",
    "\\end{pmatrix}$,\n",
    "что в свою очередь равняется $\\frac{\\partial \\vec f}{\\partial \\vec g}\\frac{\\partial \\vec g}{\\partial x}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial \\vec f}{\\partial x} = \\frac{\\partial \\vec f}{\\partial \\vec g}\\frac{\\partial \\vec g}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы можно было дифференциировать сложную функцию многих переменных достаточно в данном правиле заменить $x$ на $\\vec x$. Тогда полное правило дифференциирования сложной функции многих переменных в векторно-матричной форме примет вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\vec f}{\\partial \\vec x}= \n",
    "\\frac{\\partial \\vec f}{\\partial \\vec g}\\frac{\\partial \\vec g}{\\partial \\vec x}=\n",
    "\\begin{pmatrix} \n",
    "    \\frac{\\partial f_1}{\\partial g_1} & \\frac{\\partial f_1}{\\partial g_2} & ... & \\frac{\\partial f_1}{\\partial g_k} \\\\\n",
    "    \\frac{\\partial f_2}{\\partial g_1} & \\frac{\\partial f_2}{\\partial g_2} & ... & \\frac{\\partial f_2}{\\partial g_k} \\\\\n",
    "    ... & ... & ... & ... \\\\\n",
    "    \\frac{\\partial f_m}{\\partial g_1} & \\frac{\\partial f_m}{\\partial g_2} & ... & \\frac{\\partial f_m}{\\partial g_k} \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "    \\frac{\\partial g_1}{\\partial x_1} & \\frac{\\partial g_1}{\\partial x_2} & ... & \\frac{\\partial g_1}{\\partial x_n} \\\\\n",
    "    \\frac{\\partial g_2}{\\partial x_1} & \\frac{\\partial g_2}{\\partial x_2} & ... & \\frac{\\partial g_2}{\\partial x_n} \\\\\n",
    "    ... & ... & ... & ... \\\\\n",
    "    \\frac{\\partial g_k}{\\partial x_1} & \\frac{\\partial g_k}{\\partial x_2} & ... & \\frac{\\partial g_k}{\\partial x_n} \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix} \n",
    "    \\sum\\limits_{i=1}^{k}\\frac{\\partial f_1}{\\partial g_k}\\frac{\\partial g_k}{\\partial x_1} &\n",
    "    \\sum\\limits_{i=1}^{k}\\frac{\\partial f_2}{\\partial g_k}\\frac{\\partial g_k}{\\partial x_2} &\n",
    "    ... &\n",
    "    \\sum\\limits_{i=1}^{k}\\frac{\\partial f_m}{\\partial g_k}\\frac{\\partial g_k}{\\partial x_n} \\\\    \n",
    "    \\sum\\limits_{i=1}^{k}\\frac{\\partial f_1}{\\partial g_k}\\frac{\\partial g_k}{\\partial x_1} &\n",
    "    \\sum\\limits_{i=1}^{k}\\frac{\\partial f_2}{\\partial g_k}\\frac{\\partial g_k}{\\partial x_2} &\n",
    "    ... &\n",
    "    \\sum\\limits_{i=1}^{k}\\frac{\\partial f_m}{\\partial g_k}\\frac{\\partial g_k}{\\partial x_n} \\\\    \n",
    "    ... & ... & ... & ... \\\\    \n",
    "    \\sum\\limits_{i=1}^{k}\\frac{\\partial f_1}{\\partial g_k}\\frac{\\partial g_k}{\\partial x_1} &\n",
    "    \\sum\\limits_{i=1}^{k}\\frac{\\partial f_2}{\\partial g_k}\\frac{\\partial g_k}{\\partial x_2} &\n",
    "    ... &\n",
    "    \\sum\\limits_{i=1}^{k}\\frac{\\partial f_m}{\\partial g_k}\\frac{\\partial g_k}{\\partial x_n} \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где m - количество функций f, n - количество переменных x, k - количество функций g. Результирующая матрица Якоби будет иметь размер m на n, так как приведенные матрицы имеют размер m на k и k на n. Зачастую функции $f$ и $g$ зависят от одной соответствующей переменной x, при этом матрицы становятся квадратными. Поэтому вычисления производных в векторно-матричной форме можно значительно упростить, т.к. для их проведения зачастую бывает достаточно диагональной матрицы, а не матрицы Якоби. Как было показано выше если функция $y_i$ не содержит $z_j$ то $\\frac{\\partial y}{\\partial z}$=0, а матрица Якоби превращается в диагональню. По аналогии если $f_i$ зависит только от $g_i$, а $g_i$ от $x_i$, то:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial \\vec f}{\\partial \\vec g} = diag(\\frac{\\partial f_i}{\\partial g_i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial \\vec g}{\\partial \\vec x} = diag(\\frac{\\partial g_i}{\\partial x_i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial \\vec f}{\\partial \\vec x}=diag(\\frac{\\partial f_i}{\\partial g_i})diag(\\frac{\\partial g_i}{\\partial x_i})=diag(\\frac{\\partial f_i}{\\partial g_i}\\frac{\\partial g_i}{\\partial x_i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таком случае матрица Якоби превращается в диагональную, элементами которой являются производные сложной функции одной переменной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обощив все вышесказанное можно составить таблицу, описывающую все возможные варианты цепного правила."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=1000 border=\"1\">\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">\n",
    "            $\\frac{\\partial}{\\partial \\vec x} \\vec f(\\vec g(\\vec x))=\\frac{\\partial \\vec f}{\\partial \\vec x}\\frac{\\partial \\vec g}{\\partial \\vec x}$\n",
    "        </td>\n",
    "        <td colspan=\"2\" align=\"center\">\n",
    "            $x$\n",
    "        </td>\n",
    "        <td>\n",
    "            $\\vec x$\n",
    "        </td>\n",
    "    </tr>            \n",
    "    <tr>\n",
    "        <td>\n",
    "            $g$\n",
    "        </td>        \n",
    "        <td>\n",
    "            $\\vec g$\n",
    "        </td>\n",
    "        <td>\n",
    "            $\\vec g$\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            $f$\n",
    "        </td>        \n",
    "        <td>\n",
    "            $\\frac{\\partial f}{\\partial g}\\frac{\\partial g}{\\partial x}$\n",
    "        </td>\n",
    "        <td>\n",
    "            $\\frac{\\partial f}{\\partial \\vec g}\\frac{\\partial \\vec g}{\\partial x}=            \n",
    "            (\\frac{\\partial f}{\\partial g_1}, \\frac{\\partial f}{\\partial g_2}, ..., \\frac{\\partial f}{\\partial g_k})\n",
    "            \\begin{pmatrix} \n",
    "                \\frac{\\partial g_1}{\\partial x} \\\\ \n",
    "                \\frac{\\partial g_2}{\\partial x} \\\\ \n",
    "                ... \\\\\n",
    "                \\frac{\\partial g_k}{\\partial x} \\\\ \n",
    "            \\end{pmatrix}            \n",
    "            $\n",
    "        </td>\n",
    "        <td>\n",
    "            $\\frac{\\partial f}{\\partial \\vec g}\\frac{\\partial \\vec g}{\\partial \\vec x}=            \n",
    "            (\\frac{\\partial f}{\\partial g_1}, \\frac{\\partial f}{\\partial g_2}, ..., \\frac{\\partial f}{\\partial g_k})*\n",
    "            \\begin{pmatrix} \n",
    "                \\frac{\\partial g_1}{\\partial x_1} & \\frac{\\partial g_1}{\\partial x_2} & ... & \\frac{\\partial g_1}{\\partial x_n} \\\\ \n",
    "                \\frac{\\partial g_2}{\\partial x_1} & \\frac{\\partial g_2}{\\partial x_2} & ... & \\frac{\\partial g_2}{\\partial x_n} \\\\ \n",
    "                ... & ... & ... & ... \\\\\n",
    "                \\frac{\\partial g_k}{\\partial x_1} & \\frac{\\partial g_k}{\\partial x_2} & ... & \\frac{\\partial g_k}{\\partial x_n} \\\\ \n",
    "            \\end{pmatrix}            \n",
    "            $\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            $\\vec f$\n",
    "        </td>        \n",
    "        <td>\n",
    "            $\\frac{\\partial \\vec f}{\\partial g}=\n",
    "            \\begin{pmatrix} \n",
    "                \\frac{\\partial f_1}{\\partial g} \\\\ \n",
    "                \\frac{\\partial f_2}{\\partial g} \\\\ \n",
    "                ... \\\\\n",
    "                \\frac{\\partial f_m}{\\partial g} \\\\ \n",
    "            \\end{pmatrix}\n",
    "            \\frac{\\partial g}{\\partial x}\n",
    "            $\n",
    "        </td>\n",
    "        <td>\n",
    "            $\\frac{\\partial \\vec f}{\\partial \\vec g}\\frac{\\partial \\vec g}{\\partial x}=            \n",
    "            \\begin{pmatrix} \n",
    "                \\frac{\\partial f_1}{\\partial g_1} & \\frac{\\partial f_1}{\\partial g_2} & ... & \\frac{\\partial f_1}{\\partial g_k} \\\\ \n",
    "                \\frac{\\partial f_2}{\\partial g_1} & \\frac{\\partial f_2}{\\partial g_2} & ... & \\frac{\\partial f_2}{\\partial g_k} \\\\ \n",
    "                ... & ... & ... & ... \\\\\n",
    "                \\frac{\\partial f_m}{\\partial g_1} & \\frac{\\partial f_m}{\\partial g_2} & ... & \\frac{\\partial f_m}{\\partial g_k} \\\\ \n",
    "            \\end{pmatrix}\n",
    "            \\begin{pmatrix} \n",
    "                \\frac{\\partial g_1}{\\partial x} \\\\ \n",
    "                \\frac{\\partial g_2}{\\partial x} \\\\ \n",
    "                ... \\\\\n",
    "                \\frac{\\partial g_k}{\\partial x} \\\\ \n",
    "            \\end{pmatrix}            \n",
    "            $\n",
    "        </td>\n",
    "        <td>\n",
    "            $\\frac{\\partial \\vec f}{\\partial \\vec g}\\frac{\\partial \\vec g}{\\partial \\vec x}=            \n",
    "            \\begin{pmatrix} \n",
    "                \\frac{\\partial f_1}{\\partial g_1} & \\frac{\\partial f_1}{\\partial g_2} & ... & \\frac{\\partial f_1}{\\partial g_k} \\\\ \n",
    "                \\frac{\\partial f_2}{\\partial g_1} & \\frac{\\partial f_2}{\\partial g_2} & ... & \\frac{\\partial f_2}{\\partial g_k} \\\\ \n",
    "                ... & ... & ... & ... \\\\\n",
    "                \\frac{\\partial f_m}{\\partial g_1} & \\frac{\\partial f_m}{\\partial g_2} & ... & \\frac{\\partial f_m}{\\partial g_k} \\\\ \n",
    "            \\end{pmatrix}*\n",
    "            \\begin{pmatrix} \n",
    "                \\frac{\\partial g_1}{\\partial x_1} & \\frac{\\partial g_1}{\\partial x_2} & ... & \\frac{\\partial g_1}{\\partial x_n} \\\\ \n",
    "                \\frac{\\partial g_2}{\\partial x_1} & \\frac{\\partial g_2}{\\partial x_2} & ... & \\frac{\\partial g_2}{\\partial x_n} \\\\ \n",
    "                ... & ... & ... & ... \\\\\n",
    "                \\frac{\\partial g_k}{\\partial x_1} & \\frac{\\partial g_k}{\\partial x_2} & ... & \\frac{\\partial g_k}{\\partial x_n} \\\\ \n",
    "            \\end{pmatrix}            \n",
    "            $\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Вычисление частной производной матричного умножения</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример: пусть имееются матрицы A и B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "A = \n",
    "\\begin{pmatrix} \n",
    "     a_{11} & a_{12} \\\\\n",
    "     a_{21} & a_{22} \\\\\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "B = \n",
    "\\begin{pmatrix} \n",
    "     b_{11} & b_{12} \\\\\n",
    "     b_{21} & b_{22} \\\\\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если $C = AB$, то $C$ примет вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "С = \n",
    "\\begin{pmatrix} \n",
    "     a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\\\\n",
    "     a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} \\\\\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсюда получаем:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial C}{\\partial a_{11}}=\n",
    "\\begin{pmatrix} \n",
    "     b_{11} & b_{12} \\\\\n",
    "     0 & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial C}{\\partial a_{12}}=\n",
    "\\begin{pmatrix} \n",
    "     b_{21} & b_{22} \\\\\n",
    "     0 & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial C}{\\partial a_{21}}=\n",
    "\\begin{pmatrix} \n",
    "     0 & 0 \\\\\n",
    "     b_{11} & b_{12} \\\\\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial C}{\\partial a_{22}}=\n",
    "\\begin{pmatrix} \n",
    "     0 & 0 \\\\\n",
    "     b_{21} & b_{22} \\\\\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда получаем $\\frac{\\partial C}{\\partial A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial C}{\\partial A}=\n",
    "\\begin{pmatrix} \n",
    "     \\sum\\frac{\\partial C}{\\partial a_{11}} & \\sum\\frac{\\partial C}{\\partial a_{12}} \\\\\n",
    "     \\sum\\frac{\\partial C}{\\partial a_{21}} & \\sum\\frac{\\partial C}{\\partial a_{22}} \\\\\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix} \n",
    "     b_{11} + b_{12} &  b_{21} + b_{22}\\\\\n",
    "     b_{11} + b_{12} &  b_{21} + b_{22}\\\\\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае данные вычисления примут вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial C}{\\partial A}=\n",
    "\\begin{pmatrix} \n",
    "     \\sum\\frac{\\partial C}{\\partial a_{11}} & \\sum\\frac{\\partial C}{\\partial a_{12}} \\\\\n",
    "     \\sum\\frac{\\partial C}{\\partial a_{21}} & \\sum\\frac{\\partial C}{\\partial a_{22}} \\\\\n",
    "\\end{pmatrix} \\frac{\\partial A}{\\partial A}= B^T \\frac{\\partial A}{\\partial A}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где матрица $C = AB$ (матричное умножение). Аналогично можно найти формулу расчетов для $\\frac{\\partial C}{\\partial B}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial C}{\\partial B}=\n",
    "\\begin{pmatrix} \n",
    "     \\sum\\frac{\\partial C}{\\partial b_{11}} & \\sum\\frac{\\partial C}{\\partial b_{12}} \\\\\n",
    "     \\sum\\frac{\\partial C}{\\partial b_{21}} & \\sum\\frac{\\partial C}{\\partial b_{22}} \\\\\n",
    "\\end{pmatrix} \\frac{\\partial B}{\\partial B}=  \\frac{\\partial B}{\\partial B} A^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Градиентный спуск в нейронных сетях</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь имеются все необходимые элементы для рассчета градиентного спуска в нейронных сетях. Простейшая нейронная сеть имеет вид $z(x)=actiavation(\\vec w \\vec x + b)$, где $\\vec x$ - вектор признаков, $\\vec w$ - веса нейронной сети, $b$ - смещение нейронной сети, $\\vec w, b$ - обучаемые параметры, $z(x)=activation(x)=max(0, x)$ (функция активации ReLU). Тогда $z(x)=max(0, \\vec w \\vec x + b)$. Оптимизация данной нейронной сети сводится к оптимизации параметров $\\vec w, b$ с помощью алгоритма градментного спуска. Для этого необходимо рассчитать производные $\\frac{\\partial z(x)}{\\partial \\vec w}$ и $\\frac{\\partial z(x)}{\\partial b}$. Для того чтобы определить насколько сильно нейронная сеть ошибается необходимо ввести функцию потерь (loss function). Для примера возьмем функцию потерь $l(x)=\\frac{1}{N}\\sum\\limits_{i}(z(x) - y_i)^2$. Для вычисления данных производных необходимо определить порядок действий $l(x)=loss(max(0, sum(dot(W, x), b)))$. Чтобы найти частные производные необходимо уметь вычислять производные следующих операций:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* вычисление производной суммы\n",
    "* вычисление производной скалярного произведения векторов\n",
    "* вычисление производной max\n",
    "* вычисление производной функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Вычисление частных производных суммы</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как было описано выше частные производные суммы имеют следующий вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial \\sum\\limits_{i} (x_i) }{\\partial x_1} = \\sum\\limits_{i=1}^n \\frac{\\partial x_i}{\\partial x_1} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial \\sum\\limits_{i} (x_i) }{\\partial \\vec x} = \n",
    "\\big(\\sum\\limits_{i=1}^n \\frac{\\partial x_i}{\\partial x_1}, \\sum\\limits_{i=1}^n \\frac{\\partial x_i}{\\partial x_2}, ..., \\sum\\limits_{i=1}^n \\frac{\\partial x_i}{\\partial x_n}  \\big) = \n",
    "(1, 1, ... 1) = \\vec 1 ^ T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial \\sum\\limits_{i} (k_ix_i) }{\\partial \\vec x} = \n",
    "\\big(\\sum\\limits_{i=1}^n \\frac{\\partial k_ix_i}{\\partial x_1}, \\sum\\limits_{i=1}^n \\frac{\\partial k_ix_i}{\\partial x_2}, ..., \\sum\\limits_{i=1}^n \\frac{\\partial k_ix_i}{\\partial x_n}  \\big) = \n",
    "(k_1, k_2, ... k_n) = \\vec k_i ^ T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Вычисление частных производных скалярного произведения векторов</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скалярное произведение $\\vec w \\vec x$ можно представить в виде $\\sum\\limits_{i}w_ix_i$. Тогда:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial \\sum\\limits_{i}w_ix_i}{\\partial \\vec w} = (x_1, x_2, ... x_n)^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Вычисление частных производных функции активации</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как было описанно выше производная функции max:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\frac{\\partial}{\\partial x}max(f, g)=\n",
    "\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   \\frac{\\partial f}{\\partial x} \\ ,если\\ f > g\\\\\n",
    "   \\frac{\\partial g}{\\partial x} \\ ,если\\ f \\leq g\\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для случая $max(0, \\vec w \\vec x+b)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\frac{\\partial}{\\partial \\vec w}max(0, \\vec w \\vec x + b)=\n",
    "\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   \\vec 0 \\ ,если\\ 0 > \\vec w \\vec x + b \\\\\n",
    "   \\vec x^T \\ ,если\\ 0 \\leq \\vec w \\vec x + b\\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\frac{\\partial}{\\partial b}max(0, \\vec w \\vec x + b)=\n",
    "\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   \\vec 0 \\ ,если\\ 0 > \\vec w \\vec x + b \\\\\n",
    "   1 \\ ,если\\ 0 \\leq \\vec w \\vec x + b\\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Вычисление частных производных функции потерь</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь имеет следующий вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$l(x) = \\frac{1}{N}\\sum\\limits_{i}(y_i - z(x))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial l}{\\partial x} = \\frac{1}{N} \\frac{\\partial}{\\partial x}\\sum\\limits_{i}(y_i - z(x, \\vec w, b))^2 = \\frac{1}{N} \\sum\\limits_{i} \\frac{\\partial}{\\partial x}(y_i - z(x, \\vec w, b))^2 = \\frac{1}{N} \\sum\\limits_{i} 2(y_i - z(x, \\vec w ,b)) \\frac{\\partial}{\\partial x} (y_i - z(x, \\vec w, b)) = \\frac{1}{N} \\sum\\limits_{i} 2(y_i - z(x, \\vec w ,b)) \\frac{\\partial}{\\partial x} (- z(x, \\vec w, b))) = \\frac{1}{N} \\sum\\limits_{i} 2(z(x, \\vec w ,b) - y_i) \\frac{\\partial z(x, \\vec w, b)}{\\partial x} )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial l}{\\partial \\vec w} = \\frac{1}{N} \\sum\\limits_{i} 2(z(x, \\vec w ,b) - y_i) \\frac{\\partial z(x, \\vec w, b)}{\\partial \\vec w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial l}{\\partial w} = \\frac{1}{N} \\sum\\limits_{i} 2(z(x, \\vec w ,b) - y_i) \\frac{\\partial z(x, \\vec w, b)}{\\partial b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассчитаем производную $\\frac{\\partial l}{\\partial \\vec w}$ до конца:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial l}{\\partial \\vec w} = \\frac{1}{N} \\sum\\limits_{i} 2(z(x, \\vec w ,b) - y_i) \\frac{\\partial z(x, \\vec w, b)}{\\partial \\vec w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\frac{\\partial z(x, \\vec w, b)}{\\partial \\vec w}=\n",
    "\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   \\vec 0 \\ ,если\\ 0 > \\vec w \\vec x + b \\\\\n",
    "   \\vec x^T \\ ,если\\ 0 \\leq \\vec w \\vec x + b\\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда для случая $0 > \\vec w \\vec x + b$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial l}{\\partial \\vec w} = \\frac{1}{N} \\sum\\limits_{i} 2(z(x, \\vec w ,b) - y_i) \\vec 0 = \\vec 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для случая $0 \\leq \\vec w \\vec x + b$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial l}{\\partial \\vec w} = \\frac{1}{N} \\sum\\limits_{i} 2(z(x, \\vec w ,b) - y_i) ( \\vec x)^T = \\frac{2}{N} \\sum\\limits_{i} (z(x, \\vec w ,b) - y_i) \\vec x ^ T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсюда получаем:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial l}{\\partial \\vec w}=\n",
    "\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   \\vec 0 \\ ,если\\ 0 > \\vec w \\vec x + b \\\\\n",
    "   \\frac{2}{N} \\sum\\limits_{i} (z(x, \\vec w ,b) - y_i) \\vec x ^ T \\ ,если\\ 0 \\leq \\vec w \\vec x + b\\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично получаем:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial l}{\\partial b}=\n",
    "\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   0 \\ ,если\\ 0 > \\vec w \\vec x + b \\\\\n",
    "    \\frac{2}{N} \\sum\\limits_{i} (z(x, \\vec w ,b) - y_i) \\ ,если\\ 0 \\leq \\vec w \\vec x + b\\\\\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Градиентный спуск</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зная частные производные $\\frac{\\partial l}{\\partial \\vec w}$ и $\\frac{\\partial l}{\\partial b}$ можно оптимизировать эти параметры с помощью градиентного спуска. Для этого необхожимо давать небольшие приращения для $w$ и $b$ в направлении противоположном градиенту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec w_{t+1} = \\vec w_t - \\alpha \\frac{\\partial l}{\\partial \\vec w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b_{t+1} = b_t - \\alpha \\frac{\\partial l}{\\partial \\vec b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где $\\alpha$ - коэффициент скорости обучения. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Операции дифференциирования в тензорно-матричной форме форме </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для рассчета некоторых операций связанных с нейронными сетями необходимо обобщить операции дифференциирования в векторно-матричной форме до операций над многомерными матрицами и разработать для них спецефические операции и правила их дифференциирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Различие понятий матрицы, многомерные матрицы, тензора </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для рассчетов глубоких нейронных зачастую требуется проводить операции над объектами с размерностью более двух. В математике такие объекты называются тензорами. Но для расчета нейронных сетей зачастую необходимо иметь возможность обрабатывать и хранить матрицы в сложных иерархических структурах и проводит над этими матрицами операции. Самым банальным примером такой работы является набор фильтров в CNN. Поэтому в нейронных сетях используются многомерные матрицы. По сути это многомерные массивы, которые хранят матрицы и имеют ряд спецефических операций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* операция индексирования\n",
    "* операция увеличения размерности\n",
    "* операция уменьшения размерности\n",
    "* операция транспонирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как при расчете нейронных сетей не применяются тензорные операции, то здесь и далее все многомерные объекты хранящие данные (многомерные матрицы), для простоты описания, будут называться тензорами. А все операции над ними будут производится по правилам многомерных матриц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "возможно стоит добавить рисунки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Дифференциирование операции увеличения размерности (broadcasting) </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В математике не существует операции увеличения размерности матрицы(broadcasting), тем боле не вычисляется ее производная. Но для работы системы автоматического вычисления производных необходимо уметь данную операцию выполнять и рассчитывать производную для этой операции. Приведем пример такой операции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим операцию прибавления числа $x = 3$ к матрице $A = \n",
    "\\begin{pmatrix} \n",
    "     1 & 2 \\\\\n",
    "     3 & 4 \\\\\n",
    "\\end{pmatrix}$. На самом деле данная операция равнозначна следующей цепочки действий:\n",
    "* $x = 3$ превращается в матрицу $X = \n",
    "\\begin{pmatrix} \n",
    "     3 & 3 \\\\\n",
    "     3 & 3 \\\\\n",
    "\\end{pmatrix}$, где размер $X$ равен размеру $A$.\n",
    "* производится сложение $A$ и $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расширение $x$ до $X$ называется broadcasting. Операция увеличения размерности создает для матрицы или вектора дополнительные измерения (ячейки памяти) и копирует в них соответствующие значения. Например: имеется вектор $v = (1,2)$ необходимо расширить его до тензора размера (3, 2, 2). Для этого размер вектора надо представить в виде (1, 1, 2). Тогда расширенный тензор примет вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ T = \n",
    "\\begin{pmatrix} \n",
    "    \\begin{pmatrix} \n",
    "        1 & 2 \\\\\n",
    "        1 & 2 \\\\\n",
    "    \\end{pmatrix} \\\\\n",
    "    \\begin{pmatrix} \n",
    "        1 & 2 \\\\\n",
    "        1 & 2 \\\\\n",
    "    \\end{pmatrix} \\\\\n",
    "    \\begin{pmatrix} \n",
    "        1 & 2 \\\\\n",
    "        1 & 2 \\\\\n",
    "    \\end{pmatrix} \\\\  \n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу в общем виде: имеется тензор $T_1$ размерностью n = $(d_1, d_2, ..., d_n)$   необходимо расширить до тензор $T_2$ размера m = $(d_1, d_2, ..., d_m)$. То есть $T_2 = broadcast(T_1)$. Необходимо вычислить производную $\\frac{\\partial broadcast(T_1)}{\\partial T_1}$. Данную операцию возможно произвести если:\n",
    "* i-е измерения $d_i$ тензоров $T_1$ и $T_2$ совпадают;\n",
    "* одно из i-х измерений тензоров $T_1$ и $T_2$ равно единице или не задано явно. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда финальная версия операции в общем виде принимает вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$T_2[d_1][d_2]...[d_{m-n}] = T_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где $d_1,\\ d_2,\\ ...\\ d_n$ - несовпавшие измерения $T_1$ и $T_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда операция дифференциирования данной операция сводится к суммированию ячеек тензора по определенным осям. Поэтому производная тензора $T_1$ с размерами $(d_1, d_2, ..., d_n)$  по тензору $T_2$ с размерами $(d_1, d_2, ... d_m)$ - $\\frac{\\partial T}{\\partial X}$ примет вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial T}{\\partial X}[d_1][d_3][d_5]...[d_n] = \\sum\\limits_{i\\ по\\ d_2,j\\ по d_4...}T_2[d_1][i][d_3][j]...[d_m] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где $d_2,\\ d_4,\\ ...$ измерения для суммирования, которые есть в $T_2$, но отсутствуют в $T_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример дифференциирования операции увеличения размерности:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим имеется вектор $\\vec v = \n",
    " \\begin{pmatrix} \n",
    "        sin(x) \\\\\n",
    "        cos(x) \\\\\n",
    " \\end{pmatrix}$ после операции увеличения размерности до матрицы размера 2*2 он превратиться в матрицу $M = \n",
    " \\begin{pmatrix} \n",
    "        sin(x) & sin(x) \\\\\n",
    "        cos(x) & cos(x) \\\\\n",
    " \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для дифференциирования данной операции необходимо проссумировать строки матрицы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial M}{\\partial v} = \\frac{\\partial M}{\\partial brodcasing}\\frac{\\partial broadcasting}{\\partial v} = \n",
    "\\begin{pmatrix} \n",
    "        2sin(x) \\\\\n",
    "        2cos(x) \\\\\n",
    " \\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "        cos(x) \\\\\n",
    "        -sin(x) \\\\\n",
    " \\end{pmatrix} =\n",
    "\\begin{pmatrix} \n",
    "        2sin(x)cos(x) \\\\\n",
    "        -2cos(x)sin(x) \\\\\n",
    " \\end{pmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Дифференциирование операции уменьшения размерности (суммирования) </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процесс расчетов может возникнуть необходимость уменьшить количество измерений тензора, например после операции суммирования его элементов по некоторым осям. Данная операция имеет вид: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ T_1[d_1][d_3][d_5]...[d_n] = \\sum\\limits_{i\\ по\\ d_2,j\\ по d_4...}T_2[d_1][i][d_3][j]...[d_m] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где $T_1$ и $T_2$ - тензоры, $d_2$, $d_4$ и т.д. измерения для суммирования, $i$, $j$, ... индексы для суммирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если $T_1$ = $sum(T_2)$, то $\\frac{\\partial sum(T_2)}{\\partial X}$ примет вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial sum(T_2)}{\\partial X} = broadcast(T_2, X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где операция $broadcast(T_2, X)$ - операция увеличения размерности тензора от размеров $T_2$ до размеров $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть имеется матрица $M = \n",
    " \\begin{pmatrix} \n",
    "        sin(x) \\ sin(x) \\\\\n",
    "        cos(x) \\ cos(x) \\\\\n",
    " \\end{pmatrix}$ после применения операции уменьшения размерности ($zip$) получается вектор \n",
    " $\\vec v = \n",
    " \\begin{pmatrix} \n",
    "        sin(x) \\\\\n",
    "        cos(x) \\\\\n",
    " \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда для дифференциирования операции уменьшения размерности необходимо применить операцию увеличения размерности, то есть:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial M}{\\partial \\vec v} = \\frac{\\partial M}{\\partial broadcast}\\frac{\\partial broadcast}{\\partial \\vec v} = broadcast(\\vec v \\rightarrow M) \\frac{\\partial broadcast}{\\partial M} = \n",
    " \\begin{pmatrix} \n",
    "        sin(x) & sin(x) \\\\\n",
    "        cos(x) & cos(x) \\\\\n",
    " \\end{pmatrix}\n",
    " \\begin{pmatrix} \n",
    "        cos(x) & cos(x) \\\\\n",
    "        -sin(x) & -sin(x) \\\\\n",
    " \\end{pmatrix} = \n",
    " \\begin{pmatrix} \n",
    "        sin(x)cos(x) & sin(x)cos(x) \\\\\n",
    "        -sin(x)cos(x) & -sin(x)cos(x) \\\\\n",
    " \\end{pmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Дифференциирование операции приведения размера </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для дальнейших расчетов, особенно при дифференциировании операции матричного умножения многомерных матриц потребуется операция приведения размера. Дифференциирование операции приведения размера включает в себя использование двцх операций: дифференциирования опеации увелечения размера и дифференциирование операции уменьшения размера. Она имеет следующий псевдокод:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "norm(A, B)\n",
    "    если многомерная матрица A имеет меньший, чем B размер,\n",
    "        то используем операцию broadcasting для приведения размеров\n",
    "    если многомерная матрица A имеет больший размер, чем B,\n",
    "        то используем операцию уменьшения размерности для приведения размеров\n",
    "    если размеры равны,\n",
    "        то ничего не делаем.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Дифференциирование операции индексации</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример: пусть имеем матрицу $F=\n",
    "\\begin{pmatrix} \n",
    "     f_{11}(X) & f_{12}(X) \\\\\n",
    "     f_{21}(X) & f_{22}(X) \\\\\n",
    "\\end{pmatrix}$ и матрицу переменных $X =\n",
    "\\begin{pmatrix} \n",
    "     x_{11} & x_{12} \\\\\n",
    "     x_{21} & x_{22} \\\\\n",
    "\\end{pmatrix}$,рассмотрим чему будет равна производная операции $X[1]$. Операция $X[1]$ вернет 1-ю строку таблицы, т.е. $X[1] = (f_{11}(X)\\ f_{12}(X))$.Тогда $\\frac{\\partial F}{\\partial X} = (\\frac{\\partial f_{11}}{\\partial X}\\ \\frac{\\partial f_{12}}{\\partial X})=(\\frac{\\partial f_{11}}{\\partial x_{11}}+\\frac{\\partial f_{11}}{\\partial x_{12}}+\\frac{\\partial f_{11}}{\\partial x_{21}}+\\frac{\\partial f_{11}}{\\partial x_{22}};\\ \\frac{\\partial f_{12}}{\\partial x_{11}}+\\frac{\\partial f_{12}}{\\partial x_{12}}+\\frac{\\partial f_{12}}{\\partial x_{21}}+\\frac{\\partial f_{11}}{\\partial x_{22}}).\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично можно показать, что для каждого элемента результирующей матрицы  $X_{1n}$ выполняется равенство:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X[i_1][i_2]=(\\frac{\\partial f_{11} }{\\partial x_{i_1i_2}} + \\frac{\\partial f_{12} }{\\partial x_{i_1i_2}}+ ... +\\frac{\\partial f_{mn-1} }{\\partial x_{i_1i_2}} + \\frac{\\partial f_{mn} }{\\partial x_{i_1i_2}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где $n, m$ - размерности соответствующих измерений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае для $n$-мерного тензора $X=index(T)$ производная индексирования примет вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X[i_1][i_2]...[i_n] = 0,$$ если упорядоченный кортеж $(i_1, i_2, ..., i_n)$ не содержится в наборе индексов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X[i_1][i_2]...[i_n] = \\sum\\limits_{}\\frac{\\partial f[i_1][i_2]...[i_n]}{\\partial x[j_1][j_2][j_n]},$$ если упорядоченный кортеж $(i_1, i_2, ..., i_n)$ содержится в наборе индексов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть имеем матрицу \n",
    "$M = \n",
    "\\begin{pmatrix} \n",
    "     sin(x) & cos(x) \\\\\n",
    "     sin(x) & cos(x) \\\\\n",
    "\\end{pmatrix}$ \n",
    "и матрицу переменных \n",
    "$X =\n",
    "\\begin{pmatrix} \n",
    "     x & x \\\\\n",
    "     x & x \\\\\n",
    "\\end{pmatrix}$\n",
    ", где все переменные $x$ совпадают.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда операция \n",
    "$M[1] = \n",
    "\\begin{pmatrix} \n",
    "     sin(x) & cos(x) \\\\\n",
    "\\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассчитаем чему равна производная: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial M[1]}{\\partial X} = \\frac{\\partial M[1]}{\\partial M}\\frac{\\partial M}{\\partial X} = \n",
    "\\begin{pmatrix} \n",
    "     sin(x) & cos(x) \\\\\n",
    "     0 & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "     cos(x) & -sin(x) \\\\\n",
    "     0 & 0 \\\\\n",
    "\\end{pmatrix} \n",
    "=\n",
    "\\begin{pmatrix} \n",
    "     sin(x)cos(x) & -sin(x)cos(x) \\\\\n",
    "     0 & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Дифференциирование унарных операций в тензорной форме</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дифференциирование унарных операций в тензорной форме в значительной степени похоже на дифференциирование в векторно-матричной форме, что облегчает вычисления. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial T}{\\partial X}[d_1][d_2][...][d_n] = \\frac{\\partial T_{d_1d_2...d_n}}{\\partial X_{d_1d_2...d_n}}\\frac{\\partial X}{\\partial X}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть имеем \n",
    "$Y = \n",
    "\\begin{pmatrix} \n",
    "     sin(x) & cos(y) \\\\\n",
    "     tg(z) & ctg(p) \\\\\n",
    "\\end{pmatrix}$ \n",
    "и матрицу\n",
    "$X = \n",
    "\\begin{pmatrix} \n",
    "     x & y \\\\\n",
    "     z & p \\\\\n",
    "\\end{pmatrix}$, необходимо найти частную производную\n",
    "$$\\frac{Y}{X} = \n",
    "\\begin{pmatrix} \n",
    "     cos(x) & -sin(y) \\\\\n",
    "     \\frac{1}{cos^2(z)} & \\frac{1}{-sin^2(p)} \\\\\n",
    "\\end{pmatrix}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Дифференциирование бинарных операций в тензорной форме</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дифференциирование бинарных операций в тензорной форме тоже очень похоже на векторно-матричную форму."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сложения имеет вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial (X+Y)}{\\partial X} = T(1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial (X+Y)}{\\partial Y} = T(1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где T(1) - тензор заполненый единицами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для вычитания имеет вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial (X-Y)}{\\partial X} = T(1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial (X-Y)}{\\partial Y} = T(-1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для умножения имеет вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial (X*Y)}{\\partial X} = Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial (X*Y)}{\\partial Y} = X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для деления имеет вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial (X/Y)}{\\partial X} = \\frac{1}{Y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial (X/Y)}{\\partial Y} = -\\frac{X}{Y^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть имеем \n",
    "$A = \n",
    "\\begin{pmatrix} \n",
    "     sin(x) & sin(x) \\\\\n",
    "     sin(x) & sin(x) \\\\\n",
    "\\end{pmatrix}$ \n",
    ",\n",
    "$B = \n",
    "\\begin{pmatrix} \n",
    "     cos(x) & cos(x) \\\\\n",
    "     cos(x) & cos(x) \\\\\n",
    "\\end{pmatrix}$\n",
    ",\n",
    "$X = \n",
    "\\begin{pmatrix} \n",
    "     x & x \\\\\n",
    "     x & x \\\\\n",
    "\\end{pmatrix}$, \n",
    "$F(X) = A + B$, необходимо найти производную $\\frac{\\partial F}{\\partial X}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial F}{\\partial X} = \\frac{\\partial F}{\\partial sum}\\frac{\\partial sum}{\\partial X} = \\frac{\\partial F}{\\partial A}\\frac{\\partial A}{\\partial X} + \\frac{\\partial F}{\\partial B}\\frac{\\partial B}{\\partial X} = \n",
    "\\begin{pmatrix} \n",
    "     cos(x) & cos(x) \\\\\n",
    "     cos(x) & cos(x) \\\\\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix} \n",
    "     -sin(x) & -sin(x) \\\\\n",
    "     -sin(x) & -sin(x) \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} \n",
    "     cos(x) - sin(x) & cos(x) - sin(x) \\\\\n",
    "     cos(x) - sin(x) & cos(x) - sin(x) \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Дифференциирование умножения многомерных матриц в тензорной форме </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для расчетов производной матричного умножения в тензорной форме необходимо представить данную операцию в виде набора нескольких более простых операций. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* операцию транспонирования многомерных матриц\n",
    "* операцию изменения размерности\n",
    "* операцию матричного умножения (для многомерных матриц)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда операцию дифференциирования умножения многомерных матриц $\\frac{\\partial (M_1*M_2)}{\\partial M_1}$ и $\\frac{\\partial (M_1*M_2)}{\\partial M_2}$ (не путать с операцией поэлементного умножения) можно представить в виде:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial (M_1*M_2)}{\\partial M_1} = norm(M_2^T)\\frac{\\partial M_1}{\\partial M_1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial (M_1*M_2)}{\\partial M_2} = norm(M_1^T)\\frac{\\partial M_2}{\\partial M_2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где функция norm является функцией приведения размера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если операция дифференциирования умножения многомерных матриц в тензорной форме происходит на некотором этапе работы алгоритма, то формулы расчктов примут вид:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial (M_1*M_2)}{\\partial M_1} = norm(T * M_2^T)\\cdot\\frac{\\partial M_1}{\\partial M_1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial (M_1*M_2)}{\\partial M_2} = norm(M_1^T * T)\\cdot\\frac{\\partial M_2}{\\partial M_2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где операция $*$ - матричное умножение, а операция $\\cdot$ - поэлементное умножение, многомерная матрица T - результат, полученный с предыдущего шага алгоритма рассчета градиента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Градиентный спуск в нейронных сетях на основе цепного правила в тензорной форме</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск в тензорной форме имеет такойй же вид, как и в матричной с одним отличием: вместо матриц в нем применяются многомерные матрицы и операции над ними."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\vec w_{t+1} = \\vec w_t - \\alpha \\frac{\\partial l}{\\partial \\vec w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где $\\alpha$ - коэффициент скорости обучения, $\\vec w_{t}$ - итерация шага $t$, $l$ - рассчетная функция."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Разработка фреймворка глубокого обучения</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве иллюстрации всех описанных идей разработаем фреймворк машинного обучения подобный PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Архитектура </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном проекте будет 4 основных вида объектов:\n",
    "* тензор - основной класс фреймворка, отвечает за всю основную обработку\n",
    "* операции - классы отвечающие за конкреиные реализации операций\n",
    "* функции - функции, которые дают интерфейс для доступа к некоторым операциям в виде функций, а не методов класса\n",
    "* модели - слои нейронных сетей и другие модели необходимые для фреймворка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный фремворк написан в учебных целях и как демонстрация математических принципов лежащих в основе работы нейронных сетей, поэтому в нем используется малое уоличество приемов оптимизации и численных алгоритмов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Тензор</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем рассмотрение данного фреймворка с онсновного класса тензора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для корректной работы необходимо подкоючить следующие заголовочные файлы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конструктор класса имеет следующий вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "\n",
    "    def __init__(self, data, calc_grad=False):\n",
    "        self._data = np.array(data)\n",
    "\n",
    "        self._old_shape = None\n",
    "        if self._data.ndim == 0:\n",
    "            self._old_shape = self._data.shape\n",
    "            self._data = self._data[None, None]\n",
    "        elif self._data.ndim == 1:\n",
    "            self._old_shape = self._data.shape\n",
    "            self._data = self._data[..., None]\n",
    "\n",
    "        self._calc_grad = calc_grad\n",
    "        self._operation = operations.VariableOperation(self)\n",
    "        self._grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс Tensor содержит слелующие основные поля:\n",
    "* _data - данные хранимые в виде numpy array\n",
    "* _calc_grad - переменная, показывающая необходимость расчета градиента\n",
    "* _operation - операция, пораждающая данный тензор\n",
    "* _grad - градиент рассчитанный для данного тензора (если такой расчет производился)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        if self._old_shape is not None:\n",
    "            return self._data.reshape(self._old_shape)\n",
    "        else:\n",
    "            return self._data\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def matrix(self):\n",
    "        return self._data\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def calc_grad(self):\n",
    "        return self._calc_grad\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def grad(self):\n",
    "        if self._grad is not None:\n",
    "            if self._old_shape is not None:\n",
    "                return Tensor(self._grad._data.reshape(self._old_shape), calc_grad=self._calc_grad)\n",
    "            else:\n",
    "                return self._grad\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    @property\n",
    "    def grad_matrix(self):\n",
    "        return self._grad\n",
    "\n",
    "\n",
    "    @grad_matrix.setter\n",
    "    def grad_matrix(self, grad):\n",
    "        self._grad = grad\n",
    "\n",
    "\n",
    "    @property\n",
    "    def T(self):\n",
    "        return self.t()\n",
    "\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы data и matrix дают доступ к данным, но метод matrix возвращает данные всегда приведенные в наиболее удобную для расчетов форму. Метод data всегда возвращает данные в том виде, в котором они были заданны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данная проблема вызвана тем, что для расчетов вектора должны быть в форме \n",
    "$\\vec v = \\begin{pmatrix} \n",
    "     x_1 \\\\\n",
    "     x_2 \\\\\n",
    "     ... \\\\\n",
    "     x_n\n",
    "\\end{pmatrix}$, а массивы зачастую храняться и задаются в виде $\\vec v = (x_1, x_2, ... c_n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:  \n",
    "\n",
    "    def forward(self):\n",
    "        return self._operation.forward()\n",
    "\n",
    "    def backward(self, tensor=None):\n",
    "        if self.calc_grad:\n",
    "            self._operation.backward(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операция forward вечает за вычисление операции порадившей тензор. Операция backward отвечает за расчет обратного хода алгоритма градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для упрощения кода и объединения всех однотипных действий используется функция __prepare_operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \n",
    "    def __prepare_operation(self, operation, other_grad=False):\n",
    "        data = operation.forward()\n",
    "        data._operation = operation\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее идут унарные операции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \n",
    "    def sqr(self):\n",
    "        op = operations.SqrOperation(self)\n",
    "        return self.__prepare_operation(op)\n",
    "\n",
    "    def sqrt(self):\n",
    "        op = operations.SqrtOperation(self)\n",
    "        return self.__prepare_operation(op)\n",
    "\n",
    "    def exp(self):\n",
    "        op = operations.ExpOperation(self)\n",
    "        return self.__prepare_operation(op)\n",
    "\n",
    "    def sigmoid(self):\n",
    "        op = operations.SigmoidOperation(self)\n",
    "        return self.__prepare_operation(op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операция транспонирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \n",
    "    def t(self):\n",
    "        op = operations.TransposeOperation(self)\n",
    "        return self.__prepare_operation(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операция суммирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " class Tensor:\n",
    "    \n",
    "    def sum(self):\n",
    "        op = operations.SumOperation(self)\n",
    "        return self.__prepare_operation(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операции hardmax и softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \n",
    "    def softmax(self):\n",
    "        op = operations.SoftMaxOperation(self)\n",
    "        return self.__prepare_operation(op)\n",
    "\n",
    "    def hardmax(self):\n",
    "        op = operations.HardMaxOperation(self)\n",
    "        return self.__prepare_operation(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операция индексирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        op = operations.IndexOperation(self, item)\n",
    "        return self.__prepare_operation(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бинарные операции поиска минимума и максимума."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \n",
    "    def min(self, other):\n",
    "        op = operations.MinOperation(self, other)\n",
    "        return self.__prepare_operation(op)\n",
    "\n",
    "\n",
    "    def max(self, other):\n",
    "        op = operations.MaxOperation(self, other)\n",
    "        return self.__prepare_operation(op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бинарные операции: +, -, /, *."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "\n",
    "    def __add__(self, other):\n",
    "        op = operations.AddOperation(self, other)\n",
    "        return self.__prepare_operation(op)\n",
    "\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        op = operations.SubOperation(self, other)\n",
    "        return self.__prepare_operation(op)\n",
    "\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        op = operations.MulOperation(self, other)\n",
    "        return self.__prepare_operation(op)\n",
    "\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        op = operations.DivOperation(self, other)\n",
    "        return self.__prepare_operation(op)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операция матричного умножения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " def __matmul__(self, other):\n",
    "        op = operations.MatMulOperation(self, other)\n",
    "        return self.__prepare_operation(op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Утилитарные функции </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном разделе представленны все утилитарные функции, использующиеся в фреймворке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для вычислений потребуются функции сравнения размеров двух тензоров. Функции shape_greater и shape_less сравнивает два размера тензоров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_greater(shape1, shape2):\n",
    "    if len(shape1) == len(shape2):\n",
    "        if shape1 > shape2:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif len(shape1) > len(shape2):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def shape_less(shape1, shape2):\n",
    "    if len(shape1) == len(shape2):\n",
    "        if shape1 < shape2:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif len(shape1) < len(shape2):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Транспонирование матрицы, тензора и многомерной матрицы отличаются в значительной степени. Основной операцие фреймворка является транспонирование многомерной матрицы. Оно отличается от транспонирования вектора тем, что меняются местами только два последних измерения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_transpose(np_tensor):\n",
    "    if np_tensor.ndim == 0:\n",
    "        return np.array([[np_tensor]])\n",
    "    elif np_tensor.ndim == 1:\n",
    "        return np.expand_dims(np_tensor)\n",
    "    elif np_tensor.ndim == 2:\n",
    "        return np_tensor.T\n",
    "    else:\n",
    "        return np.swapaxes(np_tensor, -1, -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее идет операция уменьшения размерности многомерной матрицы за счет суммирования по координатам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_zip_variable(np_tensor, np_same_as):\n",
    "    shape_in = np_tensor.shape\n",
    "    shape_out = np_same_as.shape\n",
    "\n",
    "    if shape_in == shape_out:\n",
    "        return np_tensor\n",
    "    else:\n",
    "        res = []\n",
    "        for i, (si, so) in enumerate(itertools.zip_longest(shape_in[::-1], shape_out[::-1])):\n",
    "            if si != so:\n",
    "                res.append(-i - 1)\n",
    "        return np.sum(np_tensor, axis=tuple(res)).reshape(np_same_as.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операция увеличения размерности матрицы за счет операции broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_extend(np_tensor, np_same_as):\n",
    "    return np.broadcast_arrays(np_tensor, np_same_as)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализация размера матрицы для выполнения бинарных операций при обратном течении градиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_reshape(np_tensor, np_same_as):\n",
    "    shape1 = np_tensor.shape\n",
    "    shape2 = np_same_as.shape\n",
    "    if shape1 == shape2:\n",
    "        return np_tensor\n",
    "    elif shape_greater(shape1, shape2):\n",
    "        return grad_zip_variable(np_tensor, np_same_as)\n",
    "    else:\n",
    "        return grad_extend(np_tensor, np_same_as)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Операции над тензорами</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном разделе будут приведены коды непосредственного дифференциирования различных операций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корневой класс операции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._res = None\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс для обработки переменных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableOperation(Operation):\n",
    "\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "\n",
    "    def forward(self):\n",
    "        self._res = Tensor(self._x.matrix, self._x.calc_grad)\n",
    "        return self._res\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        if self._x.calc_grad:\n",
    "            if self._x.grad_matrix is None:\n",
    "                if np_tensor is None:\n",
    "                    self._x.grad_matrix = Tensor(np.ones_like(self._x.matrix))\n",
    "                else:\n",
    "                    self._x.grad_matrix = Tensor(grad_reshape(np_tensor, self._x.matrix))\n",
    "            else:\n",
    "                if np_tensor is None:\n",
    "                    self._x.grad_matrix = Tensor(grad_reshape(np.ones_like(self._x.matrix) + self._x.matrix, self._x.matrix))\n",
    "                else:\n",
    "                    self._x.grad_matrix = Tensor(grad_reshape(self._x.grad_matrix.matrix + np_tensor, self._x.matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс для обработки транспонирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransposeOperation(Operation):\n",
    "\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "\n",
    "    def forward(self):\n",
    "        self._res = Tensor(self._x.matrix.T, self._x.calc_grad)\n",
    "        return self._res\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        if self._x.calc_grad:\n",
    "            if np_tensor is None:\n",
    "                self._x.backward(np.ones_like(self._x.matrix))\n",
    "            else:\n",
    "                self._x.backward(np_tensor.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс для обработки операции суммирования тензора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumOperation(Operation):\n",
    "\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "\n",
    "    def forward(self):\n",
    "        self._res = Tensor((np.sum(self._x.matrix),), self._x.calc_grad)\n",
    "        return self._res\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        if self._x.calc_grad:\n",
    "            tensor = np.ones_like(self._x.matrix)\n",
    "            if np_tensor is None:\n",
    "                self._x.backward(grad_reshape(np_tensor, self._x.matrix))\n",
    "            else:\n",
    "                self._x.backward(grad_reshape(np_tensor, self._x.matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс для обработки операции softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxOperation(Operation):\n",
    "\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "\n",
    "    def forward(self):\n",
    "        exp = self._x.exp()\n",
    "        sum = exp.sum()\n",
    "        self._res = exp / sum\n",
    "        return Tensor(self._res.matrix, self._res.calc_grad)\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        if self._x.calc_grad:\n",
    "            self._res.backward(np_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс для обработки операции hardmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardMaxOperation(Operation):\n",
    "\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "\n",
    "    def forward(self):\n",
    "        sum = self._x.sum()\n",
    "        self._res = self._x / sum\n",
    "        return self._x / sum\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        if self._x.calc_grad:\n",
    "            self._res.backward(np_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс для обработки операции возведения в квадрат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqrOperation(Operation):\n",
    "\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "\n",
    "    def forward(self):\n",
    "        self._res = Tensor(self._x.matrix * self._x.matrix, self._x.calc_grad)\n",
    "        return self._res\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        if self._x.calc_grad:\n",
    "            data = 2 * self._x.matrix\n",
    "            if np_tensor is None:\n",
    "                self._x.backward(data)\n",
    "            else:\n",
    "                self._x.backward(np_tensor * data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс для извлечения квадратного корня."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqrtOperation(Operation):\n",
    "\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "\n",
    "    def forward(self):\n",
    "        self._res = Tensor(np.sqrt(self._x.matrix), self._x.calc_grad)\n",
    "        return self._res\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        if self._x.calc_grad:\n",
    "            data = 0.5 / np.sqrt(self._x.matrix)\n",
    "            if np_tensor is None:\n",
    "                self._x.backward(data)\n",
    "            else:\n",
    "                self._x.backward(np_tensor * data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс для вычисления экспоненты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpOperation(Operation):\n",
    "\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "\n",
    "    def forward(self):\n",
    "        self._res = Tensor(np.exp(self._x.matrix), self._x.calc_grad)\n",
    "        return self._res\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        if self._x.calc_grad:\n",
    "            data = np.exp(self._x.matrix)\n",
    "            if np_tensor is None:\n",
    "                self._x.backward(data)\n",
    "            else:\n",
    "                self._x.backward(np_tensor * data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс для обработки сигмоиды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidOperation(Operation):\n",
    "\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "\n",
    "    def forward(self):\n",
    "        ex = np.exp(self._x.matrix)\n",
    "        self._res = Tensor(ex / (ex + 1), self._x.calc_grad)\n",
    "        return self._res\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        if self._x.calc_grad:\n",
    "            data = self._res.matrix * (1 - self._res.matrix)\n",
    "            if np_tensor is None:\n",
    "                self._x.backward(data)\n",
    "            else:\n",
    "                self._x.backward(np_tensor * data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка операции поиска минимума двух многомерных матриц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinOperation(Operation):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "\n",
    "    def forward(self):\n",
    "        self._res = tensor.Tensor(np.minimum(self._x.matrix, self._y.matrix), self._x.calc_grad or self._y.calc_grad)\n",
    "        return self._res\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        if self._x.calc_grad:\n",
    "            if np_tensor is None:\n",
    "                one = np.ones_like(self._res.matrix)\n",
    "                res = np.zeros_like(self._res.matrix)\n",
    "                idx = self._x.matrix <= self._y.matrix\n",
    "                res[idx] = one[idx]\n",
    "                self._x.backward(grad_reshape(res, self._x.matrix))\n",
    "            else:\n",
    "                res = np.zeros_like(np_tensor)\n",
    "                idx = self._x.matrix <= self._y.matrix\n",
    "                res[idx] = np_tensor[idx]\n",
    "                self._x.backward(grad_reshape(res, self._x.matrix))\n",
    "        if self._y.calc_grad:\n",
    "            if np_tensor is None:\n",
    "                one = np.ones_like(self._res.matrix)\n",
    "                res = np.zeros_like(self._res.matrix)\n",
    "                idx = self._x.matrix > self._y.matrix\n",
    "                res[idx] = one[idx]\n",
    "                self._y.backward(grad_reshape(res, self._y.matrix))\n",
    "            else:\n",
    "                res = np.zeros_like(np_tensor)\n",
    "                idx = self._x.matrix > self._y.matrix\n",
    "                res[idx] = np_tensor[idx]\n",
    "                self._y.backward(grad_reshape(res, self._y.matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка операции поиска максимума двух многомерных матриц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxOperation(Operation):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "\n",
    "    def forward(self):\n",
    "        self._res = Tensor(np.maximum(self._x.matrix, self._y.matrix), self._x.calc_grad or self._y.calc_grad)\n",
    "        return self._res\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        if self._x.calc_grad:\n",
    "            if np_tensor is None:\n",
    "                one = np.ones_like(self._res.matrix)\n",
    "                res = np.zeros_like(self._res.matrix)\n",
    "                idx = self._x.matrix >= self._y.matrix\n",
    "                res[idx] = one[idx]\n",
    "                self._x.backward(grad_reshape(res, self._x.matrix))\n",
    "            else:\n",
    "                res = np.zeros_like(np_tensor)\n",
    "                idx = self._x.matrix >= self._y.matrix\n",
    "                res[idx] = np_tensor[idx]\n",
    "                self._x.backward(grad_reshape(res, self._x.matrix))\n",
    "        if self._y.calc_grad:\n",
    "            if np_tensor is None:\n",
    "                one = np.ones_like(self._res.matrix)\n",
    "                res = np.zeros_like(self._res.matrix)\n",
    "                idx = self._x.matrix < self._y.matrix\n",
    "                res[idx] = one[idx]\n",
    "                self._y.backward(grad_reshape(res, self._y.matrix))\n",
    "            else:\n",
    "                res = np.zeros_like(np_tensor)\n",
    "                idx = self._x.matrix < self._y.matrix\n",
    "                res[idx] = np_tensor[idx]\n",
    "                self._y.backward(grad_reshape(res, self._y.matrix))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка операции сложения двух многомерных матриц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddOperation(Operation):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "\n",
    "    def forward(self):\n",
    "        self._res = Tensor(self._x.matrix + self._y.matrix, self._x.calc_grad or self._y.calc_grad)\n",
    "        return self._res\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        if self._x.calc_grad:\n",
    "            if np_tensor is None:\n",
    "                self._x.backward(np.ones_like(self._res.matrix))\n",
    "            else:\n",
    "                self._x.backward(grad_reshape(np_tensor, self._x.matrix))\n",
    "        if self._y.calc_grad:\n",
    "            if np_tensor is None:\n",
    "                self._y.backward(np.ones_like(self._res.matrix))\n",
    "            else:\n",
    "                self._y.backward(grad_reshape(np_tensor, self._y.matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка операции поиска разности двух многомерных матриц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubOperation(Operation):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "\n",
    "    def forward(self):\n",
    "        self._res = Tensor(self._x.matrix - self._y.matrix, self._x.calc_grad or self._y.calc_grad)\n",
    "        return self._res\n",
    "\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        if self._x.calc_grad:\n",
    "            if np_tensor is None:\n",
    "                self._x.backward(np.ones_like(self._res.matrix))\n",
    "            else:\n",
    "                self._x.backward(grad_reshape(np_tensor, self._x.matrix))\n",
    "        if self._y.calc_grad:\n",
    "            if np_tensor is None:\n",
    "                self._y.backward(- np.ones_like(self._res.matrix))\n",
    "            else:\n",
    "                self._y.backward(grad_reshape(- np_tensor, self._y.matrix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка операции произведения двух многомерных матриц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulOperation(Operation):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "\n",
    "    def forward(self):\n",
    "        self._res = Tensor(self._x.matrix * self._y.matrix, self._x.calc_grad or self._y.calc_grad)\n",
    "        return self._res\n",
    "\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        if self._x.calc_grad:\n",
    "            if np_tensor is None:\n",
    "                self._x.backward(grad_reshape(self._y.matrix, self._x.matrix))\n",
    "            else:\n",
    "                self._x.backward(grad_reshape(np_tensor * self._y.matrix, self._x.matrix))\n",
    "        if self._y.calc_grad:\n",
    "            if np_tensor is None:\n",
    "                self._y.backward(grad_reshape(self._x.matrix, self._y.matrix))\n",
    "            else:\n",
    "                self._y.backward(grad_reshape(np_tensor * self._x.matrix, self._y.matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка операции деления двух многомерных матриц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DivOperation(Operation):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "\n",
    "    def forward(self):\n",
    "        self._res = Tensor(self._x.matrix / self._y.matrix, self._x.calc_grad or self._y.calc_grad)\n",
    "        return self._res\n",
    "\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        if self._x.calc_grad:\n",
    "            if np_tensor is None:\n",
    "                self._x.backward(grad_reshape(1 / self._y.matrix, self._x.matrix))\n",
    "            else:\n",
    "                self._x.backward(grad_reshape(np_tensor / self._y.matrix, self._x.matrix))\n",
    "        if self._y.calc_grad:\n",
    "            if np_tensor is None:\n",
    "                self._y.backward(grad_reshape(- self._x.matrix / (self._y.matrix * self._y.matrix), self._y.matrix))\n",
    "            else:\n",
    "                self._y.backward(grad_reshape(np_tensor * (-self._x.matrix / (self._y.matrix * self._y.matrix)), self._y.matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка операции матричного умножения двух многомерных матриц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMulOperation(Operation):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "\n",
    "    def forward(self):\n",
    "        self._res = Tensor(self._x.matrix @ self._y.matrix, self._x.calc_grad or self._y.calc_grad)\n",
    "        return self._res\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        if self._x.calc_grad:\n",
    "            if np_tensor is None:\n",
    "                self._x.backward(grad_reshape(grad_transpose(self._y.matrix), self._x.matrix))\n",
    "            else:\n",
    "                self._x.backward(grad_reshape(np_tensor @ grad_transpose(self._y.matrix), self._x.matrix))\n",
    "        if self._y.calc_grad:\n",
    "            if np_tensor is None:\n",
    "                self._y.backward(grad_reshape(grad_transpose(self._x.matrix), self._y.matrix))\n",
    "            else:\n",
    "                self._y.backward(grad_reshape(grad_transpose(self._x.matrix) @ np_tensor, self._y.matrix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка операции индексирования двух многомерных матриц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexOperation(Operation):\n",
    "    def __init__(self, x, index):\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "        self._index = index\n",
    "\n",
    "    def forward(self):\n",
    "        zero = np.zeros_like(self._x.matrix)\n",
    "        data = self._x.matrix[self._index]\n",
    "        zero[self._index] = data\n",
    "        self._res = Tensor(zero)\n",
    "        return Tensor(data, self._x.calc_grad)\n",
    "\n",
    "    def backward(self, np_tensor):\n",
    "        if self._x.calc_grad:\n",
    "            # ToDo check\n",
    "            if np_tensor is None:\n",
    "                ex_tensor = np.zeros_like(self._x.matrix)\n",
    "                ex_tensor[self._index] = np.ones_like(self._res.matrix)[self._index]\n",
    "                self._x.backward(ex_tensor)\n",
    "            else:\n",
    "                ex_tensor = np.zeros_like(self._x.matrix)\n",
    "                #!!!\n",
    "                #ex_tensor[self._index] = np_tensor.squeeze()\n",
    "                # ToDo ???\n",
    "                ex_tensor[self._index] = np_tensor #.squeeze()\n",
    "                self._x.backward(ex_tensor)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Функции </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном разделе приведены интерфейсные функции для более удобных расчетов. Все приведенные функции полностью зеркалируют соответствующие функции, приведенные в разделе с описанием класса тензора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min(x, y):\n",
    "    return x.min(y)\n",
    "\n",
    "\n",
    "def max(x, y):\n",
    "    return y.max(y)\n",
    "\n",
    "def transpose(x):\n",
    "    return x.T\n",
    "\n",
    "def sum(x):\n",
    "    return x.sum()\n",
    "\n",
    "def hardmax(x):\n",
    "    return x.hardmax()\n",
    "\n",
    "def softmax(x):\n",
    "    return x.softmax()\n",
    "\n",
    "def sqr(x):\n",
    "    return x.sqr()\n",
    "\n",
    "def sqrt(x):\n",
    "    return x.sqrt()\n",
    "\n",
    "def exp(x):\n",
    "    return x.exp()\n",
    "\n",
    "def sigmoid(x):\n",
    "    return x.sigmoid()\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "def sub(x, y):\n",
    "    return x - y\n",
    "\n",
    "def div(x, y):\n",
    "    return x / y\n",
    "\n",
    "def mul(x, y):\n",
    "    x * y\n",
    "\n",
    "def matmul(x, y):\n",
    "    return x * y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Слои нейронной сети </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разработаем на основе приведенного фреймворка нейронную сеть. Для этого необходимо создать корневой класс, от которого будут наследоваться все остальные слои сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self._parameters = []\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(args[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return self._parameters\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for i in self._parameters:\n",
    "            if i.grad is not None:\n",
    "                i.grad.matrix.fill(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разработаем на его основе линейный слой нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Model):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self._in_features = in_features\n",
    "        self._out_features = out_features\n",
    "        self._bias = bias\n",
    "\n",
    "        data = np.random.random((in_features, out_features))\n",
    "\n",
    "        self._w = Tensor(data, True)\n",
    "        self._parameters.append(self._w)\n",
    "\n",
    "        if self._bias:\n",
    "            self._b = Tensor([1.], True)\n",
    "            self._parameters.append(self._b)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self._bias:\n",
    "            return x @ self._w + self._b\n",
    "        else:\n",
    "            return x @ self._w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Пример обучения нейронной сети на данном фреймворке </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим конкретный пример обучения нейронной сети на разработанном фреймворке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [0] loss: [[8.76790461]]\n",
      "epoch: [100] loss: [[0.06511864]]\n",
      "epoch: [200] loss: [[0.00122022]]\n",
      "epoch: [300] loss: [[1.61212188e-05]]\n",
      "epoch: [400] loss: [[6.34445084e-07]]\n",
      "epoch: [500] loss: [[6.13473101e-08]]\n",
      "epoch: [600] loss: [[4.0658064e-09]]\n",
      "epoch: [700] loss: [[1.89654001e-10]]\n",
      "epoch: [800] loss: [[6.59559012e-12]]\n",
      "epoch: [900] loss: [[1.69769416e-13]]\n",
      "w= [[0.2760121 ]\n",
      " [0.72398792]]\n",
      "b= [-0.72398796]\n"
     ]
    }
   ],
   "source": [
    "from tensor import Tensor\n",
    "from model import Linear\n",
    "\n",
    "alpha = 0.001\n",
    "epochs = 1000\n",
    "\n",
    "x = [[1., 2], [2., 3.], [3., 4.], [4., 5.], [5., 6.]]\n",
    "y = [1., 2., 3., 4., 5.]\n",
    "\n",
    "x = Tensor(x)\n",
    "y = Tensor(y)\n",
    "\n",
    "m = Linear(2, 1)\n",
    "\n",
    "for i in range(epochs):\n",
    "    m.zero_grad()\n",
    "    z = (m(x) - y).sqr().sum()\n",
    "    z.backward()\n",
    "    for j in m.parameters:\n",
    "        j.update(j.matrix - alpha * j.grad.matrix)\n",
    "    if i % 100 == 0:\n",
    "        print('epoch: [{}] loss: [{}]'.format(i, z))\n",
    "\n",
    "p = m.parameters\n",
    "w = p[0]\n",
    "b = p[1]\n",
    "\n",
    "print('w=', w.data)\n",
    "print('b=', b.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
